{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maxar Image Availability Analysis\n",
    "\n",
    "The Maxar image availability workflow takes as input a list of TerraFund project ids and returns as output a csv listing every project and how much of that project’s area has Maxar imagery coverage.\n",
    "\n",
    "#### Workflow:\n",
    "1. Pull info on project characteristics for the entire portfolio using the TerraMatch API\n",
    "    - Repo/notebook: terrafund-portfolio-analysis/tm-api.ipynb\n",
    "    - Input: list of TerraFund project IDs\n",
    "    - Output: csv of all project features\n",
    "2. Using the TM API csv, pull Maxar metadata\n",
    "    - Repo/notebook: maxar-tools/decision-tree-metadata.ipynb and maxar-tools/src/decision_tree.py (? may need to change b/c of my additions to the acquire_metadata function)\n",
    "    - Input: csv of project features\n",
    "    - Output: csv of maxar metadata\n",
    "3. Create imagery features (??)\n",
    "    - Repo/notebook: terrafund-portfolio-analysis/maxar-img-avail.py\n",
    "    - Input: csv of maxar metadata and csv of TM project features\n",
    "    - Output: csv of project features and percent imagery coverage\n",
    "4. Identify projects with 100% imagery coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # keep\n",
    "import geopandas as gpd # keep\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import shape # keep\n",
    "from shapely.geometry import Polygon, Point # keep ?\n",
    "from shapely import union_all \n",
    "import ast # keep\n",
    "from datetime import datetime, timedelta # keep datetime\n",
    "import re\n",
    "import os\n",
    "import math \n",
    "import requests\n",
    "import yaml # keep\n",
    "import json\n",
    "import pyproj\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "import image_availability as img \n",
    "import process_api_results as clean\n",
    "import decision_trees as tree\n",
    "import tm_api_utils as api_request\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "tm_auth_path = '../secrets.yaml'\n",
    "tm_staging_url = \"https://api-staging.terramatch.org/research/v3/sitePolygons?\"                 # use for testing queries\n",
    "tm_prod_url = \"https://api.terramatch.org/research/v3/sitePolygons?\"                            # Use to pull data for analysis'\n",
    "approved_projects = '../terrafund-portfolio-analyses/projects_all_approved_202501091214.csv'    # List of projects with approved polygons\n",
    "feats = '../data/tm_api_TEST.csv'                                                               # Polygon metadata & geometries from TM API\n",
    "maxar_feats = '/home/darby/github_repos/maxar-tools/data/tm_api_TEST.csv'                       # Polygon metadata & geometries from TM API saved to maxar-tools repo\n",
    "maxar_md = '../data/imagery_availability/comb_img_availability_2025-02-26.csv'                  # Metadata for Maxar images corresponding to polygons\n",
    "results_path = '../data/results/'                                                                               # File path to save results to\n",
    "\n",
    "# Define filtering thesholds (stored in a dictionary)\n",
    "filters = {\n",
    "    'cloud_cover': 50,          # Remove images with >50% cloud cover\n",
    "    'off_nadir': 30,            # Remove images with >30° off-nadir angle\n",
    "    'sun_elevation': 30,        # Keep only images where sun elevation >30°\n",
    "    'date_range': (-366, 0),    # Date range of 1 year before plantstart\n",
    "    'img_count': 1,             # Threshold for identifying image availability (REASSESS)\n",
    "    'ev_range': (730,1095)      # Early verification window (2-3 years after plantstart date) (REASSESS)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Workflow Outline (DON'T RUN!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: LOAD AND PREPROCESS DATA\n",
    "# 1.1: Load polygon dataset\n",
    "poly_csv = gpd.GeoDataFrame(polygon geometries & metadata)\n",
    "\n",
    "# 1.2 Load image dataset\n",
    "img_csv = gpd.GeoDataFrame(maxar image geometries & metadata)\n",
    "\n",
    "# 1.3 Preprocess the data\n",
    "poly_gdf = preprocess_polygons(poly_csv) # Clean data, convert geometries, enforce CRS\n",
    "img_gdf = preprocess_images(img_csv) # Clean data, convert geometries, enforce CRS\n",
    "\n",
    "\n",
    "# Step 2: MERGE POLYGON DATA WITH IMAGE DATA\n",
    "merged_gdf = img_gdf.merge(poly_gdf, on=['project_id', 'poly_id'], how='left')\n",
    "\n",
    "# Step 3: PRE-FILTER IMAGES\n",
    "filtered_images = merged_gdf where:\n",
    "    (date is within allowed date range) &\n",
    "    (cloud cover < cloud_thresh) &\n",
    "    (off-nadir angle < off_nadir_thresh) &\n",
    "    (sun elevation < sun_elev_thresh)\n",
    "\n",
    "# Step 4: ITERATE THROUGH PROJECTS AND POLYGONS TO CALCULATE IMAGERY COVERAGE\n",
    "# 4.1 Create a dictionary for project-polygon mapping\n",
    "project_polygons = {project_id: list of poly_ids associated with that project} # Create a dictionary\n",
    "\n",
    "# 4.2 Initialize list to store low coverage cases\n",
    "low_img_coverage_log = []\n",
    "\n",
    "# 4.3 Iterate through each project\n",
    "for each project_id in project_polygons:\n",
    "\n",
    "    # 4.4 Get all polygons for this project\n",
    "    project_polygons_list = list of poly_ids for this project_id\n",
    "\n",
    "    # 4.5 Iterate through each polygon in the project\n",
    "    for each poly_id in project_polygons_list:\n",
    "    \n",
    "        # 4.6 Get all images associated with this polygon\n",
    "        poly_images = filtered_images[filtered_images['poly_id'] == poly_id]\n",
    "\n",
    "        # Count the number of available images\n",
    "        num_images = len(poly_images)\n",
    "\n",
    "        # If no valid image exists, record 0% coverage\n",
    "        if poly_images is empty:\n",
    "            store result: (poly_id, project_id, None, num_images, 0, 0) # No images available\n",
    "            continue\n",
    "\n",
    "        # 4.7 Select the best image (lowest cloud cover)\n",
    "        best_image = select_best_image(poly_images)\n",
    "\n",
    "        # 4.8 Get polygon and image geometries\n",
    "        poly_geom = poly_gdf[poly_gdf['poly_id'] == poly_id].geometry.iloc[0]\n",
    "        best_img_geom = best_image['img_geom']\n",
    "\n",
    "        # 4.9 Compute UTM Zone and reproject geometries\n",
    "        poly_centroid = compute centroid of poly_geom\n",
    "        utm_crs = get UTM CRS from centroid\n",
    "        poly_geom_reprojected = reproject poly_geom to utm_crs\n",
    "        best_img_geom_reprojected = reproject best_img_geom to utm_crs\n",
    "\n",
    "        # 4.10 Calculate the polygon area dynamically (in hectares)\n",
    "        poly_area_ha = poly_geom_reprojected.area / 10000\n",
    "\n",
    "        # 4.11 Calculate area of overlap\n",
    "        overlap_area = poly_geom_reprojected union best_img_geom_reprojected\n",
    "        overlap_area_ha = overlap_area / 10000\n",
    "\n",
    "        # 4.12 Compute percent of polygon area covered\n",
    "        percent_img_cover = (overlap_area / poly_area_ha) * 100\n",
    "\n",
    "        # 4.13 Log cases where imagery coverage is unexpectedly low\n",
    "        if percent_img_cover < 50:\n",
    "            log_entry = {\n",
    "                'poly_id': poly_id,\n",
    "                'project_id': project_id,\n",
    "                'best_image': best_image['title'],\n",
    "                'num_images': num_images,\n",
    "                'poly_area_ha': poly_area_ha,\n",
    "                'overlap_area_ha': overlap_area_ha,\n",
    "                'percent_img_cover': percent_img_cover\n",
    "            }\n",
    "            low_img_coverage_log.append(log_entry)\n",
    "\n",
    "        # 4.14 Store results\n",
    "        store result: (poly_id, project_id, best_image['title'], num_images, poly_area_ha, overlap_area_ha, percent_img_cover)\n",
    "\n",
    "# STEP 5: EXPORT LOW COVERAGE LOG IF NEEDED\n",
    "if low_img_coverage_log is not empty:\n",
    "    export_to_csv(low_img_coverage_log, \"low_coverage_polygons.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: LOAD & PREPROCESS DATA\n",
    "Goal: ensure input data is clean & structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.1 LOAD IN POLYGON AND IMAGE CSVS\n",
    "poly_df = pd.read_csv(feats)\n",
    "img_df = pd.read_csv(maxar_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.2 PREPROCESS POLYGON DATA\n",
    "def preprocess_polygons(poly_df, debug=False):\n",
    "    \"\"\"\n",
    "    Cleans up a dataframe of polygon metadata & geometries from the TerraMatch API and \n",
    "    converts it into a GeoDataframe\n",
    "\n",
    "    Args:\n",
    "        poly_df (DataFrame): Raw polygon dataset.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: Processed polygon dataset with a geometry column as a shapely object.\n",
    "    \"\"\"\n",
    "    # Enforce lowercase column names\n",
    "    poly_df.columns = poly_df.columns.str.lower()\n",
    "\n",
    "    # Rename 'name' and 'geometry' columns\n",
    "    poly_df = poly_df.rename(columns={'name': 'poly_name', 'geometry': 'poly_geom'})\n",
    "\n",
    "    # Convert 'plantstart' column to a datetime\n",
    "    poly_df['plantstart'] = pd.to_datetime(poly_df['plantstart'], errors='coerce')\n",
    "\n",
    "    # Convert stringified 'poly_geom' dictionaries into real dictionaries\n",
    "    poly_df['poly_geom'] = poly_df['poly_geom'].apply(lambda x: shape(ast.literal_eval(x)) if isinstance(x, str) else shape(x))\n",
    "\n",
    "    # Convert 'poly_geom' dictionaries from WKT to Shapely objects\n",
    "    poly_df['poly_geom'] = poly_df['poly_geom'].apply(shape)\n",
    "\n",
    "    # Convert to GeoDataFrame\n",
    "    poly_gdf = gpd.GeoDataFrame(poly_df, geometry='poly_geom', crs=\"EPSG:4326\")\n",
    "\n",
    "    if debug:\n",
    "        print(f\"There are {len(poly_gdf.poly_id.unique())} unique polygons for {len(poly_gdf.project_id.unique())} projects in this dataset.\")\n",
    "\n",
    "    return poly_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.3 PREPROCESS MAXAR IMAGERY DATA\n",
    "def preprocess_images(img_df, debug=True):\n",
    "    \"\"\"\n",
    "    Cleans up a dataframe of maxar image metadata & geometries from the Maxar Discovery API and \n",
    "    converts it into a GeoDataframe\n",
    "\n",
    "    Args:\n",
    "        img_df (DataFrame): Raw image metadata dataset.\n",
    "    \n",
    "    Returns: \n",
    "        GeoDataFrame: Processed image dataset with a geometry column as a shapely object.\n",
    "    \"\"\"\n",
    "    # Convert 'datetime' column to a datetime and rename\n",
    "    img_df.loc[:, 'datetime'] = pd.to_datetime(img_df['datetime'], format='%Y-%m-%dT%H:%M:%S.%fZ', errors='coerce') # Convert to datetime type\n",
    "    img_df.loc[:, 'datetime'] = img_df['datetime'].apply(lambda x: x.replace(tzinfo=None) if pd.notna(x) else x)    # Remove time zone info\n",
    "    \n",
    "    # Rename the 'datetime' column to 'img_date'\n",
    "    img_df = img_df.rename(columns={'datetime': 'img_date'}) # Rename the column img_date\n",
    "\n",
    "    # Select the relevent columns from img_df\n",
    "    img_df = img_df[['title', 'project_id', 'poly_id', 'img_date', 'area:cloud_cover_percentage', 'eo:cloud_cover', 'area:avg_off_nadir_angle', 'view:sun_elevation', 'img_geom']]\n",
    "\n",
    "    # Convert stringified 'poly_geom' dictionaries into real dictionaries\n",
    "    img_df['img_geom'] = img_df['img_geom'].apply(lambda x: shape(ast.literal_eval(x)) if isinstance(x, str) else shape(x))\n",
    "\n",
    "    # Convert 'img_geom' (image footprint geometries) from WKT to Shapely objects\n",
    "    img_df['img_geom'] = img_df['img_geom'].apply(shape)\n",
    "\n",
    "    # Convert DataFrame to GeoDataFrame\n",
    "    img_gdf = gpd.GeoDataFrame(img_df, geometry='img_geom', crs=\"EPSG:4326\")\n",
    "\n",
    "    if debug:\n",
    "        print(f\"There are {len(img_gdf)} images for {len(img_gdf.poly_id.unique())} polygons in {len(img_gdf.project_id.unique())} projects in this dataset.\")\n",
    "\n",
    "    return img_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: MERGE & FILTER DATA\n",
    "Goal: link images to polygons and apply filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.1 MERGE THE POLYGON ATTRIBUTES TO THE IMAGES GEODATAFRAME\n",
    "def merge_polygons_images(img_gdf, poly_gdf, debug=True):\n",
    "    \"\"\" \n",
    "    Merges the polygon metadata into the Maxar image GeoDataFrame. All rows of the img_gdf are preserved.\n",
    "    Also records polygons that are dropped because they don't have any associated images.\n",
    "\n",
    "    Args:\n",
    "        img_gdf (GeoDataFrame): Image metadata dataset (each row represents a Maxar image)\n",
    "        poly_gdf (GeoDataFrame): Polygon dataset (each row represents a polygon from the TM API)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (GeoDataFrame of merged dataset, list of missing polygons (poly_id, project_id))\n",
    "    \"\"\"\n",
    "    # Merge the image data with the polygon data (preserving image data rows and adding associated polygon attributes)\n",
    "    merged_gdf = img_gdf.merge(poly_gdf, on=['project_id', 'poly_id'], how='left')\n",
    "\n",
    "    # Identify polygons without any corresponding Maxar images\n",
    "    missing_polygons_df = poly_gdf[~poly_gdf['poly_id'].isin(merged_gdf['poly_id'])]\n",
    "\n",
    "    # Save poly_id and project_id of missing polygons as a list of tuples\n",
    "    missing_polygons_list = list(missing_polygons_df[['poly_id', 'project_id']].itertuples(index=False, name=None))\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Total images in img_gdf: {len(img_gdf)}\")\n",
    "        print(f\"Total polygons in poly_gdf: {len(poly_gdf)}\")\n",
    "        print(f\"Total rows in merged dataset: {len(merged_gdf)}\")\n",
    "        print(f\"Unique polygons in merged dataset: {len(merged_gdf['poly_id'].unique())}\")\n",
    "    \n",
    "        # Count polygons dropped due to no matching images\n",
    "        missing_polygons = len(poly_gdf[~poly_gdf['poly_id'].isin(merged_gdf['poly_id'])])\n",
    "        print(f\"There {missing_polygons} polygons without images in the merged dataset\")\n",
    "        print(f\"Polygons without images (dropped at this stage): {missing_polygons_list}\")\n",
    "\n",
    "    return merged_gdf, missing_polygons_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.2 FILTER IMAGES BASED ON HARD CRITERIA\n",
    "def filter_images(merged_gdf, filters, debug=True):\n",
    "    \"\"\"\n",
    "    Filters the merged dataset to retain only images that meet filters for image quality.\n",
    "    The values for the filters can be changed in the parameters section.\n",
    "\n",
    "    Args:\n",
    "        merged_gdf (GeoDataFrame): Merged dataset of images and polygons.\n",
    "        filters (dict): Dictionary containing filter thresholds (in Parameters section of notebook)\n",
    "    \n",
    "    Returns:\n",
    "        GeoDataFrame: Filtered dataset containing only the images that meet the criteria\n",
    "    \"\"\"\n",
    "    # Ensure date columns are in correct datetime format\n",
    "    merged_gdf['img_date'] = pd.to_datetime(merged_gdf['img_date'], errors='coerce')\n",
    "    merged_gdf['plantstart'] = pd.to_datetime(merged_gdf['plantstart'], errors='coerce')\n",
    "\n",
    "    # Compute the date difference (image capture date - plant start date)\n",
    "    merged_gdf['date_diff'] = (merged_gdf['img_date'] - merged_gdf['plantstart']).dt.days\n",
    "\n",
    "    # Apply filtering criteria to retain only images within the desired time range, cloud cover, \n",
    "    # off nadir angle, and sun elevation parameters\n",
    "    filtered_images = merged_gdf[\n",
    "        (merged_gdf['date_diff'] >= filters['date_range'][0]) &\n",
    "        (merged_gdf['date_diff'] <= filters['date_range'][1]) &\n",
    "        (merged_gdf['area:cloud_cover_percentage'] < filters['cloud_cover']) &\n",
    "        (merged_gdf['area:avg_off_nadir_angle'] <= filters['off_nadir']) &\n",
    "        (merged_gdf['view:sun_elevation'] >= filters['sun_elevation'])\n",
    "    ].copy()  # Copy to avoid SettingWithCopyWarning\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Total images before filtering: {len(merged_gdf)}\")\n",
    "        print(f\"Total images after filtering: {len(filtered_images)}\")\n",
    "        print(f\"Polygons with at least one valid image: {filtered_images['poly_id'].nunique()}\")\n",
    "\n",
    "    return filtered_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3: PROCESS EACH POLYGON & COMPUTE IMAGERY COVERAGE\n",
    "Goal: Prepare polygons, select the best image, and calculate imagery coverage per polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.1 GET THE BEST IMAGE FOR A GIVEN POLYGON\n",
    "def get_best_image(poly_images, debug=False):\n",
    "    \"\"\"\n",
    "    Selects the best image for a given polygon based on the lowest cloud cover. If multiple images have\n",
    "    the same cloud cover, selects the one closest to the plantstart date. \n",
    "\n",
    "    If we want to update this to include an \"expected coverage\" based on cloud cover and footprint overlap,\n",
    "    this is where it would go.\n",
    "\n",
    "    Args:\n",
    "        poly_images (GeoDataFrame): Subset of img_gdf_filtered containing images for one polygon.\n",
    "    \n",
    "    Returns:\n",
    "        GeoSeries: The best image row from poly_images\n",
    "    \"\"\"\n",
    "    # Create an absolute value date_diff column to help sort images by proximity to plantstart date\n",
    "    poly_images = poly_images.copy() # Avoid modifying the original dataframe\n",
    "    poly_images['abs_date_diff'] = poly_images['date_diff'].abs()\n",
    "\n",
    "    # Sort images by cloud cover (ascending) and then by date (closest to plantstart)\n",
    "    sorted_images = poly_images.sort_values(by=['area:cloud_cover_percentage', 'abs_date_diff'])\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\n Debug: Sorted images for this polygon (using cloud cover, then proximity to plantstart date):\")\n",
    "        print(sorted_images[['title', 'area:cloud_cover_percentage', 'img_date', 'plantstart', 'abs_date_diff']])\n",
    "\n",
    "    # Select the best image (first row after sorting)\n",
    "    best_image = sorted_images.iloc[0]\n",
    "\n",
    "    return best_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.2 GET THE APPROPRIATE UTM CRS FOR A POLYGON BASED ON ITS CENTROID\n",
    "def get_utm_crs(longitude, latitude):\n",
    "    \"\"\"\n",
    "    Determines the appropriate UTM CRS for a given polygon based on the longitude and latitude\n",
    "    of its centroid.\n",
    "\n",
    "    Args:\n",
    "        longitude (float): Longitude of the polygon centroid.\n",
    "        latitude (float): Latitude of the polygon centroid.\n",
    "    \n",
    "    Returns:\n",
    "        str: EPSG code of the best UTM CRS.\n",
    "    \"\"\"\n",
    "\n",
    "    # UTM zones range from 0 to 60, each covering 6 degrees of longitude\n",
    "    utm_zone = int((longitude + 180) / 6) + 1\n",
    "\n",
    "    # EPSG code for UTM Northern vs. Southern hemisphere\n",
    "    epsg_code = 32600 + utm_zone if latitude >= 0 else 32700 + utm_zone\n",
    "\n",
    "    return f\"EPSG:{epsg_code}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.3 COMPUTE THE IMAGERY COVERAGE FOR A GIVEN POLYGON\n",
    "def compute_polygon_image_coverage(poly_id, project_id, poly_gdf, img_gdf_filtered,\n",
    "                                   low_img_coverage_log, min_coverage_threshold=50):\n",
    "    \"\"\"\n",
    "    Computes the percentage of a polygon's area that is covered by its best available Maxar image.\n",
    "\n",
    "    Args:\n",
    "        poly_id (str): Unique polygon ID\n",
    "        project_id (str): Unique project ID\n",
    "        poly_gdf (GeoDataFrame): Polygons dataset\n",
    "        img_gdf_filtered (GeoDataFrame): Images dataset filtered by hard criteria\n",
    "        low_img_coverage_log (list): List to store cases where imagery coverage is low\n",
    "        min_coverage_threshold (int): Imagery coverage threshold below which polygons are logged\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing (poly_id, project_id, best_image, num_images, poly_area_ha,\n",
    "        overlap_area_ha, percent_img_cover)\n",
    "    \"\"\"\n",
    "\n",
    "    ## Calculate the polygon's area from its geometry\n",
    "    # Explicitly retrieve the polygon row\n",
    "    polygon_row = poly_gdf.loc[poly_gdf['poly_id'] == poly_id]\n",
    "\n",
    "    # Retrieve the actual polygon geometry\n",
    "    poly_geom = polygon_row['poly_geom'].values[0]\n",
    "\n",
    "    # Compute UTM Zone based on polygon centroid and reproject polygon and image footprint\n",
    "    poly_centroid = poly_geom.centroid\n",
    "    utm_crs = get_utm_crs(poly_centroid.x, poly_centroid.y)\n",
    "\n",
    "    # Reproject the polygon geometry into a CRS with a unit of square meters\n",
    "    poly_reprojected = gpd.GeoDataFrame(polygon_row, geometry='poly_geom', crs=\"EPSG:4326\").to_crs(utm_crs)\n",
    "    poly_geom_reprojected = poly_reprojected.geometry.iloc[0] # Extract reprojected polygon\n",
    "\n",
    "    # Calculate polygon area dynamically (in hectares)\n",
    "    poly_area_ha = poly_geom_reprojected.area / 10_000\n",
    "\n",
    "    ## Get all images associated with this polygon\n",
    "    print(f\"Computing coverage for polygon {poly_id}\")\n",
    "    poly_images = img_gdf_filtered[img_gdf_filtered['poly_id'] == poly_id]\n",
    "    num_images = len(poly_images)\n",
    "\n",
    "    ## Handle polygons with no valid images\n",
    "    # If no valid images, log the case and return 0% imagery coverage\n",
    "    if poly_images.empty:\n",
    "        log_entry = {\n",
    "            'poly_id': poly_id,\n",
    "            'project_id': project_id,\n",
    "            'best_image': None,\n",
    "            'num_images': 0,\n",
    "            'poly_area_ha': poly_area_ha,\n",
    "            'overlap_area_ha': 0,\n",
    "            'percent_img_cover': 0,\n",
    "        }\n",
    "        print(f\"Logging low covarage for polygon {poly_id} because there is no available imagery\")\n",
    "        low_img_coverage_log.append(log_entry)\n",
    "\n",
    "        # Return after logging\n",
    "        return (poly_id, project_id, None, num_images, poly_area_ha, 0, 0)\n",
    "    \n",
    "    ## If there is at least one valid image\n",
    "    # Select the best image from the filtered images for the polygon\n",
    "    best_image = get_best_image(poly_images)\n",
    "\n",
    "    # Retrieve the geometry of the best image footprint\n",
    "    best_image_geom = best_image['img_geom']\n",
    "    \n",
    "    # Reproject the best image footprint's geometry\n",
    "    best_img_reprojected = gpd.GeoDataFrame([best_image], geometry=\"img_geom\", crs=\"EPSG:4326\").to_crs(utm_crs)\n",
    "    best_img_geom_reprojected = best_img_reprojected.geometry.iloc[0]\n",
    "\n",
    "    # Compute intersection between polygon geometry and best image's footprint geometry\n",
    "    overlap_area = poly_geom_reprojected.intersection(best_img_geom_reprojected).area\n",
    "    overlap_area_ha = overlap_area / 10_000\n",
    "\n",
    "    # Compute percentage of polygon covered by best image\n",
    "    percent_img_cover = (overlap_area_ha / poly_area_ha) * 100\n",
    "\n",
    "    # Log cases where the imagery coverage of the best image is below the threshold\n",
    "    if percent_img_cover < min_coverage_threshold or percent_img_cover == 0:\n",
    "        print(f\"Logging low coverage for polygon {poly_id}: {percent_img_cover}%\")\n",
    "        log_entry = {\n",
    "            'poly_id': poly_id,\n",
    "            'project_id': project_id,\n",
    "            'best_image': best_image['title'] if best_image is not None else None,\n",
    "            'num_images': num_images,\n",
    "            'poly_area_ha': poly_area_ha,\n",
    "            'overlap_area_ha': overlap_area_ha,\n",
    "            'percent_img_cover': percent_img_cover,\n",
    "        }\n",
    "        low_img_coverage_log.append(log_entry)\n",
    "\n",
    "    # Return results\n",
    "    return (poly_id, project_id, best_image['title'], num_images, poly_area_ha, overlap_area_ha,\n",
    "            percent_img_cover)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4: COMPUTE COVERAGE PER PROJECT\n",
    "Goal: aggregate the percent imagery cover per polygon to the project level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_project_image_coverage(results_df, debug=False):\n",
    "    \"\"\"\n",
    "    Aggregates the polygon-level image coverage data to the project level.\n",
    "\n",
    "    Args:\n",
    "        results_df (DataFrame): Contains polygon-level image coverage data.\n",
    "    \n",
    "    Returns:\n",
    "        project_coverage_df (DataFrame): Aggregated project-level imagery coverage summary\n",
    "    \"\"\"\n",
    "\n",
    "    # Group data by project_id\n",
    "    grouped = results_df.groupby('project_id')\n",
    "    if debug:\n",
    "        print(f\"There are {len(grouped.project_id.unique())} projects being analyzed.\")\n",
    "    \n",
    "    # Compute summary statistics per project\n",
    "    project_coverage_df = grouped.agg(\n",
    "        num_polygons=('poly_id', 'count'), # Total polygons\n",
    "        num_polygons_with_images=('percent_img_cover', lambda x: (x > 0).sum()), # Polygons with imagery\n",
    "        num_polygons_no_images=('percent_img_cover', lambda x: (x == 0).sum()), # Polygons with 0% imagery coverage\n",
    "        total_project_area_ha=('poly_area_ha', 'sum'), # Total area of the project in hectares (sum of polygon areas)\n",
    "        total_overlap_area_ha=('overlap_area_ha', 'sum'), # Total area of the project with imagery coverage (sum of per polygon area with imagery coverage)\n",
    "    ).reset_index()\n",
    "\n",
    "    # Compute the total percent area of each project that has Maxar imagery coverage (final result)\n",
    "    project_coverage_df['total_percent_area_covered'] = (\n",
    "        (project_coverage_df['total_overlap_area_ha'] / project_coverage_df['total_project_area_ha']) * 100\n",
    "    )\n",
    "\n",
    "    return project_coverage_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 5: EXPORT RESULTS\n",
    "Goal: save results for review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.1 LOAD IN POLYGON AND IMAGE CSVS\n",
    "poly_df = pd.read_csv(feats)\n",
    "img_df = pd.read_csv(maxar_md)\n",
    "\n",
    "poly_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.2 PREPROCESS POLYGON DATA\n",
    "poly_gdf = preprocess_polygons(poly_df, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.2 PREPROCESS POLYGON DATA\n",
    "img_gdf = preprocess_images(img_df, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.1 MERGE THE POLYGON ATTRIBUTES TO THE IMAGES GEODATAFRAME\n",
    "merged_gdf, missing_polygons = merge_polygons_images(img_gdf, poly_gdf, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(missing_polygons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.1 FILTER IMAGES BASED ON HARD CRITERIA\n",
    "img_gdf_filtered = filter_images(merged_gdf, filters, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases: longitude, latitude → expected UTM zone and EPSG code\n",
    "test_coords = [\n",
    "    (-75, 40),   # Philadelphia, USA (Should be EPSG:32618)\n",
    "    (2.35, 48.85), # Paris, France (Should be EPSG:32631)\n",
    "    (120, -10),  # Indonesia (Southern Hemisphere, Should be EPSG:32751)\n",
    "    (-122, 37),  # San Francisco, USA (Should be EPSG:32610)\n",
    "    (35, 0),     # Near Equator (Should be EPSG:32636)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function on test coordinates\n",
    "for lon, lat in test_coords:\n",
    "    utm_crs = get_utm_crs(lon, lat)\n",
    "    print(f\"Longitude: {lon}, Latitude: {lat} → UTM CRS: {utm_crs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST COMPUTE POLYGON IMAGE COVERAGE\n",
    "# Initialize storage for results & low-coverage polygons list\n",
    "low_img_coverage_log = []\n",
    "results = []\n",
    "\n",
    "# Iterate through all polygons and compute imagery coverage per polygon\n",
    "for poly_id, project_id in zip(poly_gdf['poly_id'], poly_gdf['project_id']):\n",
    "    result = compute_polygon_image_coverage(poly_id, project_id, poly_gdf, img_gdf_filtered, low_img_coverage_log)\n",
    "    #print(\"Low Image Coverage Log Contents:\", low_img_coverage_log)\n",
    "    results.append(result)\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['poly_id', 'project_id', 'best_image', 'num_images',\n",
    "                                            'poly_area_ha', 'overlap_area_ha', 'percent_img_cover'])\n",
    "\n",
    "# Convert low-coverage log to DataFrame\n",
    "low_coverage_polygons_df = pd.DataFrame(low_img_coverage_log)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Processed {len(results_df)} polygons.\")\n",
    "print(f\"{len(low_coverage_polygons_df)} polygons logged as low imagery coverage.\")\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(low_img_coverage_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_coverage_polygons_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_coverage_cases = results_df[results_df['percent_img_cover'] < 50]\n",
    "print(low_coverage_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(results_df[\"percent_img_cover\"], bins=20, edgecolor=\"black\")\n",
    "plt.xlabel(\"Percent Imagery Coverage\")\n",
    "plt.ylabel(\"Number of Polygons\")\n",
    "plt.title(\"Distribution of Imagery Coverage per Polygon\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE OUT RESULTS\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "results_df.to_csv(f\"{results_path}polygon_imagery_coverage_{today}.csv\")\n",
    "\n",
    "low_coverage_polygons_df.to_csv(f\"{results_path}low_coverage_polygons_{today}.csv\")\n",
    "print(\"Saved final outputs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_results_df = aggregate_project_image_coverage(results_df, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(project_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_results_df.to_csv(f\"{results_path}project_imagery_coverage_{today}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
