{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maxar Image Availability Analysis PPC Tree Count Eligibility\n",
    "\n",
    "The Maxar image availability workflow takes as input a list of TerraFund project ids and returns as output a csv listing every project and how much of that project’s area has Maxar imagery coverage.\n",
    "\n",
    "#### Workflow:\n",
    "1. Pull info on project characteristics for the entire portfolio using the TerraMatch API\n",
    "    - Repo/notebook: terrafund-portfolio-analysis/tm-api.ipynb\n",
    "    - Input: list of TerraFund project IDs\n",
    "    - Output: csv of all project features\n",
    "2. Using the TM API csv, pull Maxar metadata\n",
    "    - Repo/notebook: maxar-tools/decision-tree-metadata.ipynb and maxar-tools/src/decision_tree.py (? may need to change b/c of my additions to the acquire_metadata function)\n",
    "    - Input: csv of project features\n",
    "    - Output: csv of maxar metadata\n",
    "3. Calculate the percent area of each project with available Maxar imagery\n",
    "    - Repo/notebook: terrafund-portfolio-analysis/maxar-img-avail.ipynb and terrafund-portfolio-analysis/src/image_coverage.py\n",
    "    - Input: csv of maxar metadata and csv of TM project features\n",
    "    - Output: csv of project features and percent imagery coverage, csv of percent imagery coverage aggregated to project level, csv of polygons with low imagery coverage\n",
    "4. Identify projects with highest imagery coverage to use for the RS image availability simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "import image_coverage as img_cover\n",
    "import analyze_img_coverage as analyze\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naming convention\n",
    "run_name = 'ppc_2025_tree_count_elig_final'\n",
    "run_dir = 'ppc_tree_count_elig'\n",
    "analysis = 'baseline' # must change if you change the date_range\n",
    "\n",
    "# Today's date\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "#today = '2025-04-02'\n",
    "\n",
    "# File paths\n",
    "feats = f'../data/{run_dir}/tm_api_{run_name}_{today}.csv' # CSV of polygon metadata & geometries from TM API (infile)\n",
    "maxar_md = f'../data/{run_dir}/imagery_availability/comb_img_availability_{run_name}_{today}.csv' # CSV of metadata for Maxar images corresponding to polygons (infile)\n",
    "task_date_ranges_path = f'../data/{run_dir}/task_date_ranges_{run_name}.csv' # CSV of the date range to search for each task\n",
    "dropped_poly_path = f'../data/{run_dir}/dropped_poly_invalid_geom_{run_name}_{today}.csv'\n",
    "results_path = f'../data/{run_dir}/results/{analysis}/' # File path to save results to\n",
    "\n",
    "# Define filtering thesholds (stored in a dictionary)\n",
    "filters = {\n",
    "    'cloud_cover': 50,           # Remove images with >50% cloud cover\n",
    "    'off_nadir': 30,             # Remove images with >30° off-nadir angle\n",
    "    'sun_elevation': 30,         # Keep only images where sun elevation >30°\n",
    "    #'date_range': (-366, 0),    # Date range of 1 year before plantstart (TerraFund baseline)\n",
    "    'date_range': (-366, 90),   # Date range of 1 year before plantstart through 3 months after (PPC baseline)\n",
    "    #'date_range': (-549, 90),    # Date range of 1.5 years before plantstart through 3 months after (modified PPC baseline)\n",
    "    #'date_range': (-732, 90),    # Date range of 2 years before plantstart through 3 months after (modified PPC baseline)\n",
    "    #'date_range': (-1586, 90),    # Date range of 4 1/3 years before plantstart through 3 months after (modified PPC baseline - CERT 2021)\n",
    "    #'date_range': (-366, 736),    # Date range of 1 year before plantstart through 2 years after (modified PPC baseline - CERT 2021)\n",
    "    #'date_range': (730, 9999),  # Date range of 2 years post-plantstart through today (upper bound of maxar_md dataset is today's date) (early verification)\n",
    "    #'date_range': (-151, 213),  # Custom all of 2022 with plantstart June 1 2022 (Rwanda, Mozambique Lidar)\n",
    "    #'date_range': (579, 883),   # Custom May - Oct 2024 with plantstart June 1 2022 (Kenya lidar)\n",
    "    #'date_range': (-59, 305),   # Custom all of 2023 with plantstart March 1 2023 (GEDI Landscapes & Global Lidar)\n",
    "    'img_count': 1,             # Threshold for identifying image availability (REASSESS)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Image Availability by Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. LOAD POLYGON AND IMAGE DATA ###\n",
    "poly_df = pd.read_csv(feats)\n",
    "img_df = pd.read_csv(maxar_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.1. PREPROCESS POLYGON DATA ###\n",
    "poly_gdf = img_cover.preprocess_polygons(poly_df, debug=False, save_dropped=True, dropped_output_path=dropped_poly_path)\n",
    "\n",
    "# Create task_id\n",
    "plant_year = pd.to_datetime(poly_gdf[\"plantstart\"], errors=\"coerce\").dt.year\n",
    "poly_gdf[\"task_id\"] = poly_gdf[\"project_id\"].astype(str) + \"_\" + plant_year.astype(\"Int64\").astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.2. PREPROCESS IMAGE DATA ###\n",
    "img_gdf = img_cover.preprocess_images(img_df, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. MERGE POLYGON METADATA INTO IMAGE DATA ###\n",
    "merged_gdf, missing_polygons_list = img_cover.merge_polygons_images(img_gdf, poly_gdf, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. FILTER IMAGES ###\n",
    "# Read in task date ranges csv\n",
    "task_ranges = pd.read_csv(task_date_ranges_path)\n",
    "\n",
    "date_range_by_task = (\n",
    "    task_ranges.set_index(\"task_id\")[[\"date_range_start\", \"date_range_end\"]]\n",
    "    .apply(lambda r: (int(r[\"date_range_start\"]), int(r[\"date_range_end\"])), axis=1)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Fallback date_range if task_id not in the csv\n",
    "DEFAULT_DATE_RANGE = filters['date_range']\n",
    "\n",
    "# Create a global filters only dictionary (cloud cover, off nadir angle, sun elevation angle)\n",
    "filters_global = {k: v for k, v in filters.items() if k != \"date_range\"}\n",
    "\n",
    "# Filter images by global filters only\n",
    "img_gdf_filtered = img_cover.filter_images(merged_gdf, filters_global, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. COMPUTE POLYGON-LEVEL IMAGERY COVERAGE ###\n",
    "# Initialize storage for results & low-coverage polygons list\n",
    "low_img_coverage_log = []\n",
    "results = []\n",
    "\n",
    "# Iterate through all polygons and compute imagery coverage per polygon\n",
    "for poly_id, project_id in zip(poly_gdf['poly_id'], poly_gdf['project_id']):\n",
    "    result = img_cover.compute_polygon_image_coverage(poly_id, project_id, poly_gdf, img_gdf_filtered, low_img_coverage_log)\n",
    "    results.append(result)\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['poly_id', 'project_id', 'best_image', 'img_date', 'num_images',\n",
    "                                            'poly_area_ha', 'overlap_area_ha', 'percent_img_cover'])\n",
    "results_df['best_image'] = results_df['best_image'].fillna(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6. AGGREGATE TO PROJECT-LEVEL COVERAGE ###\n",
    "project_results_df = img_cover.aggregate_project_image_coverage(results_df, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7. SAVE RESULTS ###\n",
    "# Percent imagery coverage by polygon\n",
    "results_df.to_csv(f\"{results_path}polygon_imagery_coverage_{run_name}_{analysis}_{today}.csv\", index=False)\n",
    "\n",
    "# Percent imagery coverage by project\n",
    "project_results_df.to_csv(f\"{results_path}project_imagery_coverage_{run_name}_{analysis}_{today}.csv\", index=False)\n",
    "\n",
    "# Polygons with low imagery coverage\n",
    "if low_img_coverage_log:\n",
    "    low_coverage_polygons_df = pd.DataFrame(low_img_coverage_log)\n",
    "    print(f\"Logging low image coverage polygons to {results_path}.\")\n",
    "    low_coverage_polygons_df['best_image'] = low_coverage_polygons_df['best_image'].fillna(\"None\")\n",
    "    low_coverage_polygons_df.to_csv(f\"{results_path}low_coverage_polygons_{run_name}_{analysis}_{today}.csv\", index=False)\n",
    "\n",
    "print(f\"Imagery coverage results saved to {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Maxar Image Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in files\n",
    "# Image availability by project\n",
    "project_img_avail = pd.read_csv(f\"{results_path}project_imagery_coverage_{run_name}_{analysis}_{today}.csv\")\n",
    "\n",
    "# Image availability by polygon\n",
    "poly_img_avail = pd.read_csv(f\"{results_path}polygon_imagery_coverage_{run_name}_{analysis}_{today}.csv\")\n",
    "\n",
    "# Low coverage polygons\n",
    "low_coverage_poly = pd.read_csv(f\"{results_path}low_coverage_polygons_{run_name}_{analysis}_{today}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Overall distribution of image availability\n",
    "# analyze.img_avail_hist(project_img_avail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # High image availability projects\n",
    "# qualifying_projects_list = analyze.count_projs_wi_img_avail(project_img_avail, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze.analyze_low_coverage_issues(low_coverage_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_cov = project_img_avail[(project_img_avail['total_percent_area_covered'] > 90) & (project_img_avail['total_percent_area_covered'] <= 101)]\n",
    "# print(len(high_cov))\n",
    "# high_cov.sort_values('total_percent_area_covered', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For PPC, calculate image availability by task (project_id + plantstart_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the 'plantstart' and 'plantstart_year' columns from poly_gdf into poly_img_avail (dataset of each polygon with associated best Maxar image)\n",
    "poly_img_avail_wi_yrs = poly_img_avail.merge(poly_gdf[['poly_id', 'plantstart']], how='left', on='poly_id')\n",
    "poly_img_avail_wi_yrs['plantstart_year'] = pd.to_datetime(poly_img_avail_wi_yrs['plantstart'], errors='coerce').dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_results_df = img_cover.aggregate_project_image_coverage_ppc(poly_img_avail_wi_yrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save percent imagery coverage by task as dataframe\n",
    "# Percent imagery coverage by polygon\n",
    "task_results_df.to_csv(f\"{results_path}task_imagery_coverage_{run_name}_{analysis}_{today}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
