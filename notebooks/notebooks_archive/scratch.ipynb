{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import json\n",
    "from shapely.geometry import shape, mapping\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "import geospatial_utils_NEW as utils\n",
    "import analyze_img_coverage as analyze\n",
    "import image_coverage as img_cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in CSVs\n",
    "# List of approved projects (with country codes)\n",
    "approved_projects = pd.read_csv('../projects_all_approved_202502211226.csv')\n",
    "\n",
    "# Polygon-level image availability\n",
    "baseline_poly = pd.read_csv('../data/tf_cohort1/results/baseline/polygon_imagery_coverage_cohort1_2025-04-02.csv')\n",
    "ev_poly = pd.read_csv('../data/tf_cohort1/results/year_2/polygon_imagery_coverage_cohort1_2025-04-02.csv')\n",
    "\n",
    "# Polygon-level low coverage \n",
    "baseline_low_cov = pd.read_csv('../data/tf_cohort1/results/baseline/low_coverage_polygons_cohort1_2025-04-02.csv')\n",
    "ev_low_cov = pd.read_csv('../data/tf_cohort1/results/year_2/low_coverage_polygons_cohort1_2025-04-02.csv')\n",
    "\n",
    "# Project-level image availability\n",
    "baseline_proj = pd.read_csv('../data/tf_cohort1/results/baseline/project_imagery_coverage_cohort1_2025-04-02.csv')\n",
    "ev_proj = pd.read_csv('../data/tf_cohort1/results/year_2/project_imagery_coverage_cohort1_2025-04-02.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check # Low-Coverage Polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF Cohort 1 Polygons\n",
    "num_poly = baseline_poly['poly_id'].nunique()\n",
    "print('# Total TF Cohort 1 Polygons:', num_poly)\n",
    "print('')\n",
    "\n",
    "# Baseline Low Coverage\n",
    "num_base_low_cov = baseline_low_cov['poly_id'].nunique()\n",
    "num_base_no_img = len(baseline_low_cov[baseline_low_cov['num_images'] == 0])\n",
    "num_base_1_img = len(baseline_low_cov[baseline_low_cov['num_images'] == 1])\n",
    "num_base_mult_img = len(baseline_low_cov[baseline_low_cov['num_images'] > 1])\n",
    "\n",
    "print('# Poly wi/ Low Cover at Baseline:', num_base_low_cov)\n",
    "print(f'{(num_base_low_cov/num_poly):.1%} of polygons have low image cover at baseline')\n",
    "print(f'Of low cover at baseline polygons, {num_base_no_img} have 0 available images.')\n",
    "print(f'    - This is {num_base_no_img/num_base_low_cov:.1%} of low-coverage polygons.')\n",
    "print(f'    - This is {num_base_no_img/num_poly:.1%} of all polygons.')\n",
    "print(f'Of low cover at baseline polygons, {num_base_1_img} have 1 available image (the low-cover image).')\n",
    "print(f'    - This is {num_base_1_img/num_base_low_cov:.1%} of low-coverage polygons.')\n",
    "print(f'    - This is {num_base_1_img/num_poly:.1%} of all polygons.')\n",
    "print(f'Of low cover at baseline polygons, {num_base_mult_img} have > 1 available images (possible other images to select from).')\n",
    "print(f'    - This is {num_base_mult_img/num_base_low_cov:.1%} of low-coverage polygons.')\n",
    "print(f'    - This is {num_base_mult_img/num_poly:.1%} of all polygons.')\n",
    "print('')\n",
    "\n",
    "# (1-Year Post-Planting) Low Coverage\n",
    "num_ev_low_cov = ev_low_cov['poly_id'].nunique()\n",
    "num_ev_no_img = len(ev_low_cov[ev_low_cov['num_images'] == 0])\n",
    "num_ev_1_img = len(ev_low_cov[ev_low_cov['num_images'] == 1])\n",
    "num_ev_mult_img = len(ev_low_cov[ev_low_cov['num_images'] > 1])\n",
    "\n",
    "print('# Poly wi/ Low Cover at EV:', num_ev_low_cov)\n",
    "print(f'{(num_ev_low_cov/num_poly):.1%} of polygons have low image cover at EV')\n",
    "print(f'Of low cover at EV polygons, {num_ev_no_img} have 0 available images.')\n",
    "print(f'    - This is {num_ev_no_img/num_ev_low_cov:.1%} of low-coverage polygons.')\n",
    "print(f'    - This is {num_ev_no_img/num_poly:.1%} of all polygons.')\n",
    "print(f'Of low cover at EV polygons, {num_ev_1_img} have 1 available image (the low-cover image).')\n",
    "print(f'    - This is {num_ev_1_img/num_ev_low_cov:.1%} of low-coverage polygons.')\n",
    "print(f'    - This is {num_ev_1_img/num_poly:.1%} of all polygons.')\n",
    "print(f'Of low cover at EV polygons, {num_ev_mult_img} have > 1 available images (possible other images to select from).')\n",
    "print(f'    - This is {num_ev_mult_img/num_ev_low_cov:.1%} of low-coverage polygons.')\n",
    "print(f'    - This is {num_ev_mult_img/num_poly:.1%} of all polygons.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_low_cover_poly_stats(all_polygons_df, low_cover_polygons_df, analysis_period):\n",
    "    \"\"\"\n",
    "    Calculate statistics about the # and % of polygons with no or low (defined as an image covering < 50% of the polygon) image coverage at a given analysis time period.\n",
    "    \n",
    "    Args:\n",
    "    - all_polygons_df (DataFrame): Dataframe of all polygons in the analysis. Must have 1 row for each unique polygon, and a unique polygon ID column called 'poly_id'.\n",
    "    - low_cover_polygons_df (DataFrame): DataFrame of all low cover polygons in the analysis. Must include a 'num_images' column containing the # of available Maxar images for that polygon\n",
    "    - analysis_period (str): A string containing the time period for this analysis (Baseline or EV). Used for naming conventions \n",
    "    \"\"\"\n",
    "    num_poly = all_polygons_df['poly_id'].nunique()\n",
    "    num_low_cov = low_cover_polygons_df['poly_id'].nunique()\n",
    "    num_no_img = len(low_cover_polygons_df[low_cover_polygons_df['num_images'] == 0])\n",
    "    num_1_img = len(low_cover_polygons_df[low_cover_polygons_df['num_images'] == 1])\n",
    "    num_mult_img = len(low_cover_polygons_df[low_cover_polygons_df['num_images'] > 1])\n",
    "\n",
    "    print(f'# Poly wi/ Low Cover at {analysis_period}:', num_low_cov)\n",
    "    print(f'{(num_low_cov/num_poly):.1%} of polygons have low image cover at {analysis_period.lower()}')\n",
    "    print(f'Of low cover at {analysis_period.lower()} polygons, {num_no_img} have 0 available images.')\n",
    "    print(f'    - This is {num_no_img/num_low_cov:.1%} of low-coverage polygons.')\n",
    "    print(f'    - This is {num_no_img/num_poly:.1%} of all polygons.')\n",
    "    print(f'Of low cover at {analysis_period.lower()} polygons, {num_1_img} have 1 available image (the low-cover image).')\n",
    "    print(f'    - This is {num_1_img/num_low_cov:.1%} of low-coverage polygons.')\n",
    "    print(f'    - This is {num_1_img/num_poly:.1%} of all polygons.')\n",
    "    print(f'Of low cover at {analysis_period.lower()} polygons, {num_mult_img} have > 1 available images (possible other images to select from).')\n",
    "    print(f'    - This is {num_mult_img/num_low_cov:.1%} of low-coverage polygons.')\n",
    "    print(f'    - This is {num_mult_img/num_poly:.1%} of all polygons.')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_low_cover_poly_stats(baseline_poly, baseline_low_cov, 'Baseline')\n",
    "calc_low_cover_poly_stats(ev_poly, ev_low_cov, 'Early Verification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(baseline_low_cov[(baseline_low_cov['overlap_area_ha'] > 0) * (baseline_low_cov['num_images'] > 1)]))\n",
    "baseline_low_cov[(baseline_low_cov['overlap_area_ha'] > 0) * (baseline_low_cov['num_images'] > 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the country codes from the list of approved projects into the csvs of image availability\n",
    "# Polygon-level\n",
    "baseline_poly = baseline_poly.merge(approved_projects[['project_id', 'country']], on='project_id', how='left')\n",
    "ev_poly = ev_poly.merge(approved_projects[['project_id', 'country']], on='project_id', how='left')\n",
    "\n",
    "# Project-level\n",
    "baseline_proj = baseline_proj.merge(approved_projects[['project_id', 'country']], on='project_id', how='left')\n",
    "ev_proj = ev_proj.merge(approved_projects[['project_id', 'country']], on='project_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by landscapes\n",
    "landscape_countries = ['BI', 'CD', 'GH', 'KE', 'RW']\n",
    "\n",
    "# Polygon-level\n",
    "baseline_poly_landscapes = baseline_poly[baseline_poly['country'].isin(landscape_countries)]\n",
    "ev_poly_landscapes = ev_poly[ev_poly['country'].isin(landscape_countries)]\n",
    "\n",
    "# Project-level\n",
    "baseline_proj_landscapes = baseline_proj[baseline_proj['country'].isin(landscape_countries)]\n",
    "ev_proj_landscapes = ev_proj[ev_proj['country'].isin(landscape_countries)]\n",
    "\n",
    "# Print results\n",
    "print(f\"There are {len(baseline_proj)} projects with imagery at baseline.\")\n",
    "print(f\"There are {len(baseline_proj_landscapes)} projects in the TF focus landscapes with imagery at baseline.\")\n",
    "print()\n",
    "print(f\"There are {len(ev_proj)} projects with imagery 1 year+ post-plantstart.\")\n",
    "print(f\"There are {len(ev_proj_landscapes)} projects in the TF focus landscapes with imagery 1 year+ post-plantstart.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FILTER BY COVERAGE THRESHOLD\n",
    "\n",
    "# Set coverage threshold\n",
    "thresh = 50\n",
    "\n",
    "# Filter projects with >= X% overage\n",
    "baseline_landscapes_thresh = baseline_proj_landscapes[baseline_proj_landscapes['total_percent_area_covered'] >= thresh]\n",
    "ev_landscapes_thresh = ev_proj_landscapes[ev_proj_landscapes['total_percent_area_covered'] >= thresh]\n",
    "\n",
    "# Print individual results\n",
    "print(f\"There are {len(baseline_landscapes_thresh)} projects with >={thresh}% coverage at baseline.\")\n",
    "print(f\"There are {len(ev_landscapes_thresh)} projects with >={thresh}% coverage 1 year+ post-planting\")\n",
    "print()\n",
    "\n",
    "# Find common project ids\n",
    "common_project_ids_thresh = set(baseline_landscapes_thresh['project_id']).intersection(ev_landscapes_thresh['project_id'])\n",
    "\n",
    "# Retrieve details of common projects\n",
    "common_projects_baseline_thresh= baseline_landscapes_thresh[baseline_landscapes_thresh['project_id'].isin(common_project_ids_thresh)]\n",
    "common_projects_ev_thresh = ev_landscapes_thresh[ev_landscapes_thresh['project_id'].isin(common_project_ids_thresh)]\n",
    "\n",
    "# Display results\n",
    "print(f\"There are {len(common_project_ids_thresh)} projects with >= {thresh}% coverage at both baseline and 1-year post-plantstart\")\n",
    "print(list(common_project_ids_thresh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create merged comparison dataframe for high coverage projects\n",
    "merged_high_cov = common_projects_baseline_thresh[['project_id', 'total_percent_area_covered']].merge(\n",
    "    common_projects_ev_thresh[['project_id', 'total_percent_area_covered']],\n",
    "    on='project_id',\n",
    "    suffixes=('_baseline', '_ev')\n",
    ")\n",
    "print(f\"\\n Coverage Comparison from Baseline to Early Verification for Projects with {thresh}% Coverage:\")\n",
    "merged_high_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create merged comparison dataframe for ALL projects\n",
    "# Find common project ids\n",
    "common_project_ids = set(baseline_proj_landscapes['project_id']).intersection(ev_proj_landscapes['project_id'])\n",
    "\n",
    "# Retrieve details of common projects\n",
    "common_projects_baseline = baseline_proj_landscapes[baseline_proj_landscapes['project_id'].isin(common_project_ids)]\n",
    "common_projects_ev = ev_proj_landscapes[ev_proj_landscapes['project_id'].isin(common_project_ids)]\n",
    "\n",
    "# Create merged comparison dataframe for all projects\n",
    "merged = common_projects_baseline[['project_id', 'total_percent_area_covered']].merge(\n",
    "    common_projects_ev[['project_id', 'total_percent_area_covered']],\n",
    "    on='project_id',\n",
    "    suffixes=('_baseline', '_ev')\n",
    ")\n",
    "\n",
    "merged.to_csv('../tf_cohort1_landscapes_baseline_ev_pct_cover_comparison.csv', index=False)\n",
    "\n",
    "print(f\"\\n Coverage Comparison from Baseline to Early Verification for All Projects in Landscapes:\")\n",
    "print(len(merged))\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TerraFund polygons Cohort 1\n",
    "polygons = pd.read_csv('../data/tf_cohort1/tm_api_cohort1_2025-04-02.csv')\n",
    "print(len(polygons))\n",
    "polygons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(polygons.plantstart.unique())[0:8]\n",
    "value = float('nan')\n",
    "print(type(value))\n",
    "for i in l:\n",
    "    if i == value:\n",
    "        print('nan')\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by ARCOS\n",
    "arcos = polygons[polygons['project_id'] == 'bbd88e69-cd85-429e-bebf-6234bf82dbb3'].copy()\n",
    "print(len(arcos))\n",
    "arcos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcos['geometry'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export ARCOS polygons as geoJSON\n",
    "utils.df_to_geojson(arcos, output_path='../arcos_polygons_2025-04-03.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the % of Polygons with >X% Coverage for Both Baseline & EV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_poly\n",
    "ev_poly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_polygon_coverage(baseline_df, ev_df, threshold):\n",
    "    # Create dataframes with only relevent columns and rename for clarity before merging\n",
    "    base = baseline_df[['poly_id', 'project_id', 'percent_img_cover']].rename(\n",
    "        columns={'percent_img_cover': 'base_pct_img_cover'})\n",
    "    ev = ev_df[['poly_id', 'percent_img_cover']].rename(\n",
    "        columns={'percent_img_cover': 'ev_pct_img_cover'})\n",
    "    \n",
    "    # Merge dataframes on poly_id\n",
    "    merged = base.merge(ev, on='poly_id', how='inner')\n",
    "\n",
    "    # Filter polygons that meet the threshold in *both* periods\n",
    "    merged['both_high'] = (\n",
    "        (merged['base_pct_img_cover'] >= threshold) &\n",
    "        (merged['ev_pct_img_cover'] >= threshold)\n",
    "    )\n",
    "\n",
    "    # Group by project and compute:\n",
    "    # - total number of shared polygons\n",
    "    # - number of polygons that meet threshold in both\n",
    "    summary = (\n",
    "        merged.groupby('project_id')\n",
    "        .agg(total_polygons=('poly_id', 'count'),\n",
    "             polygons_high_both=('both_high', 'sum'))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Add percent\n",
    "    summary['percent_polygons_high_both'] = (\n",
    "        summary['polygons_high_both'] / summary['total_polygons'] * 100\n",
    "    )\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_high_poly_cover = compare_polygon_coverage(baseline_poly_landscapes, ev_poly_landscapes, 10).sort_values(by='percent_polygons_high_both', ascending=False)\n",
    "print(len(both_high_poly_cover))\n",
    "both_high_poly_cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_high_poly_cover.to_csv('../poly_wi_gte_70_pct_cover_base_ev.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Overlap in Actual Imagery Coverage Between Baseline and EV Imagery Area "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. LOAD POLYGON AND IMAGE DATA FOR COHORT 1 (ALL TIME PERIODS) ###\n",
    "maxar_df = pd.read_csv('../data/tf_cohort1/imagery_availability/comb_img_availability_cohort1_2025-04-02.csv')\n",
    "poly_df = pd.read_csv('../data/tf_cohort1/tm_api_cohort1_2025-04-02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. PREPROCESS POLYGON AND IMAGE DATA ###\n",
    "poly_gdf = img_cover.preprocess_polygons(poly_df, debug=True)\n",
    "maxar_gdf = img_cover.preprocess_images(maxar_df, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxar_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. MERGE POLYGON METADATA INTO IMAGE DATA ###\n",
    "merged_gdf, missing_polygons_list = img_cover.merge_polygons_images(maxar_gdf, poly_gdf, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. DO INITIAL HARD FILTER OF IMAGES (INCLUDES DATE RANGE) ###\n",
    "# For Baseline\n",
    "# Set filters\n",
    "base_filters = {\n",
    "    'cloud_cover': 50,          # Remove images with >50% cloud cover\n",
    "    'off_nadir': 30,            # Remove images with >30째 off-nadir angle\n",
    "    'sun_elevation': 30,        # Keep only images where sun elevation >30째\n",
    "    'date_range': (-366, 0),    # Date range of 1 year before plantstart (baseline)\n",
    "    'img_count': 1,             # Threshold for identifying image availability (REASSESS)\n",
    "}\n",
    "# Filter gdf\n",
    "base_img_gdf_filtered = img_cover.filter_images(merged_gdf, base_filters, debug=True)\n",
    "\n",
    "# For early verification (1 year+ post plantstart)\n",
    "# Set filters\n",
    "ev_filters = {\n",
    "    'cloud_cover': 50,          # Remove images with >50% cloud cover\n",
    "    'off_nadir': 30,            # Remove images with >30째 off-nadir angle\n",
    "    'sun_elevation': 30,        # Keep only images where sun elevation >30째\n",
    "    'date_range': (365, 9999),    # Date range of y year post-plantstart through today (upper bound of maxar_md dataset is today's date) (year_2)\n",
    "    'img_count': 1,             # Threshold for identifying image availability (REASSESS)\n",
    "}\n",
    "# Filter gdf\n",
    "ev_img_gdf_filtered = img_cover.filter_images(merged_gdf, ev_filters, debug=True)\n",
    "\n",
    "# Print results\n",
    "print(f\"Total images before filtering: {len(merged_gdf)}\")\n",
    "print()\n",
    "print('BASELINE:')\n",
    "print(f\"Total images after filtering: {len(base_img_gdf_filtered)}\")\n",
    "print(f\"Polygons with at least one valid image: {base_img_gdf_filtered['poly_id'].nunique()}\")\n",
    "print()\n",
    "print('EARLY VERIFICATION:')\n",
    "print(f\"Total images after filtering: {len(ev_img_gdf_filtered)}\")\n",
    "print(f\"Polygons with at least one valid image: {ev_img_gdf_filtered['poly_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. COMPUTE POLYGON-LEVEL IMAGERY COVERAGE ###\n",
    "#### BASELINE ###\n",
    "# Initialize storage for results & low-coverage polygons list\n",
    "base_low_img_coverage_log = []\n",
    "base_results = []\n",
    "\n",
    "# Iterate through all polygons and compute imagery coverage per polygon\n",
    "for poly_id, project_id in zip(poly_gdf['poly_id'], poly_gdf['project_id']):\n",
    "    result = img_cover.compute_polygon_image_coverage(poly_id, project_id, poly_gdf, base_img_gdf_filtered, base_low_img_coverage_log)\n",
    "    base_results.append(result)\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "base_results_df = pd.DataFrame(base_results, columns=['poly_id', 'project_id', 'best_image', 'num_images',\n",
    "                                            'poly_area_ha', 'overlap_area_ha', 'percent_img_cover'])\n",
    "base_results_df['best_image'] = base_results_df['best_image'].fillna(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(base_results_df))\n",
    "base_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EARLY VERIFICATION ###\n",
    "### 5. COMPUTE POLYGON-LEVEL IMAGERY COVERAGE ###\n",
    "# Initialize storage for results & low-coverage polygons list\n",
    "ev_low_img_coverage_log = []\n",
    "ev_results = []\n",
    "\n",
    "# Iterate through all polygons and compute imagery coverage per polygon\n",
    "for poly_id, project_id in zip(poly_gdf['poly_id'], poly_gdf['project_id']):\n",
    "    result = img_cover.compute_polygon_image_coverage(poly_id, project_id, poly_gdf, ev_img_gdf_filtered, ev_low_img_coverage_log)\n",
    "    ev_results.append(result)\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "ev_results_df = pd.DataFrame(ev_results, columns=['poly_id', 'project_id', 'best_image', 'num_images',\n",
    "                                            'poly_area_ha', 'overlap_area_ha', 'percent_img_cover'])\n",
    "ev_results_df['best_image'] = ev_results_df['best_image'].fillna(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
