{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maxar Image Availability Analysis\n",
    "\n",
    "The Maxar image availability workflow takes as input a list of TerraFund project ids and returns as output a csv listing every project and how much of that projectâ€™s area has Maxar imagery coverage.\n",
    "\n",
    "#### Workflow:\n",
    "1. Pull info on project characteristics for the entire portfolio using the TerraMatch API\n",
    "    - Repo/notebook: terrafund-portfolio-analysis/tm-api.ipynb\n",
    "    - Input: list of TerraFund project IDs\n",
    "    - Output: csv of all project features\n",
    "2. Using the TM API csv, pull Maxar metadata\n",
    "    - Repo/notebook: maxar-tools/decision-tree-metadata.ipynb and maxar-tools/src/decision_tree.py (? may need to change b/c of my additions to the acquire_metadata function)\n",
    "    - Input: csv of project features\n",
    "    - Output: csv of maxar metadata\n",
    "3. Create imagery features (??)\n",
    "    - Repo/notebook: terrafund-portfolio-analysis/maxar-img-avail.py\n",
    "    - Input: csv of maxar metadata and csv of TM project features\n",
    "    - Output: csv of project features and percent imagery coverage\n",
    "4. Identify projects with 100% imagery coverage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import shape\n",
    "from shapely.geometry import Polygon, Point\n",
    "from shapely import union_all\n",
    "import ast\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import requests\n",
    "import yaml\n",
    "import json\n",
    "import pyproj\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "import image_availability as img\n",
    "import process_api_results as clean\n",
    "import decision_trees as tree\n",
    "import tm_api_utils as api_request\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "tm_auth_path = '../secrets.yaml'\n",
    "tm_staging_url = \"https://api-staging.terramatch.org/research/v3/sitePolygons?\"                 # use for testing queries\n",
    "tm_prod_url = \"https://api.terramatch.org/research/v3/sitePolygons?\"                            # Use to pull data for analysis'\n",
    "approved_projects = '../terrafund-portfolio-analyses/projects_all_approved_202501091214.csv'    # List of projects with approved polygons\n",
    "feats = '../data/tm_api_TEST.csv'                                                               # Polygon metadata & geometries from TM API\n",
    "maxar_feats = '/home/darby/github_repos/maxar-tools/data/tm_api_TEST.csv'                       # Polygon metadata & geometries from TM API saved to maxar-tools repo\n",
    "maxar_md = '../data/imagery_availability/comb_img_availability_2025-02-26.csv'                  # Metadata for Maxar images corresponding to polygons\n",
    "\n",
    "# Define thesholds\n",
    "cloud_thresh = 50             # Threshold for removing cloudy imagery\n",
    "off_nadir_thresh = 30         # Threshold for removing imagery too far off nadir\n",
    "sun_elev_thresh = 30          # Threshold for removing imagery with too steep of a sun angle\n",
    "img_count = 1                 # Threshold for identifying image availability\n",
    "baseline_range = (-366, 0)    # Baseline window (1 year before plantstart date)\n",
    "ev_range = (730, 1095)        # Early verification window (2-3 years after plant start date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load & Preprocess Data\n",
    "Inputs: \n",
    "- TM API csv\n",
    "- Maxar metadata csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load TM API polygons & convert to dataframe\n",
    "# polygons = pd.read_csv(feats)\n",
    "# polygons.columns = polygons.columns.str.lower()    # Enforce lowercase column names\n",
    "# poly_df = pd.DataFrame(polygons)\n",
    "# poly_df.columns\n",
    "\n",
    "# # Rename columns\n",
    "# poly_df = poly_df.rename(columns={'name': 'poly_name','geometry': 'poly_geom'})\n",
    "\n",
    "# # Convert 'plantstart' column to a datetime\n",
    "# poly_df['plantstart'] = pd.to_datetime(poly_df['plantstart'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TM API polygons and convert to a GeoDataFrame\n",
    "polygons = pd.read_csv(feats)\n",
    "polygons.columns = polygons.columns.str.lower()   # Enforce lowercase column names\n",
    "\n",
    "# Rename 'name' and 'geometry' columns\n",
    "poly_df = polygons.rename(columns={'name': 'poly_name', 'geometry': 'poly_geom'})  \n",
    "\n",
    "# Convert 'plantstart' column to a datetime\n",
    "poly_df['plantstart'] = pd.to_datetime(poly_df['plantstart'], errors='coerce')\n",
    "\n",
    "# Convert stringified 'poly_geom' dictionaries into real dictionaries\n",
    "poly_df['poly_geom'] = poly_df['poly_geom'].apply(lambda x: shape(ast.literal_eval(x)) if isinstance(x, str) else shape(x))\n",
    "\n",
    "# Convert 'poly_geom' (polygon geometries) from WKT to Shapely objects\n",
    "poly_df['poly_geom'] = poly_df['poly_geom'].apply(shape)\n",
    "\n",
    "# Add a field for the polygon centroid\n",
    "poly_df['poly_centroid'] = poly_df['poly_geom'].iloc[0].centroid\n",
    "\n",
    "# Convert DataFrame to GeoDataFrame\n",
    "poly_gdf = gpd.GeoDataFrame(poly_df, geometry='poly_geom', crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(poly_gdf.shape)\n",
    "poly_gdf.head()\n",
    "len(poly_gdf['poly_id'].unique())\n",
    "poly_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Maxar images metadata and convert to a GeoDataFrame\n",
    "images = pd.read_csv(maxar_md)\n",
    "print(images.columns)\n",
    "\n",
    "# Select relevent columns\n",
    "img_df = images[['title', 'project_id', 'poly_id', 'datetime', 'area:cloud_cover_percentage', 'eo:cloud_cover', 'area:avg_off_nadir_angle', 'view:sun_elevation', 'img_geom']]\n",
    "\n",
    "# Convert 'datetime' column to a datetime and rename\n",
    "img_df.loc[:, 'datetime'] = pd.to_datetime(img_df['datetime'], format='%Y-%m-%dT%H:%M:%S.%fZ', errors='coerce') # Convert to datetime type\n",
    "img_df.loc[:, 'datetime'] = img_df['datetime'].apply(lambda x: x.replace(tzinfo=None) if pd.notna(x) else x)    # Remove time zone info\n",
    "img_df = img_df.rename(columns={'datetime': 'img_date'})                                                        # Rename 'datetime' column 'img_date'\n",
    "\n",
    "# Convert stringified 'poly_geom' dictionaries into real dictionaries\n",
    "img_df['img_geom'] = img_df['img_geom'].apply(lambda x: shape(ast.literal_eval(x)) if isinstance(x, str) else shape(x))\n",
    "\n",
    "# Convert 'img_geom' (image footprint geometries) from WKT to Shapely objects\n",
    "img_df['img_geom'] = img_df['img_geom'].apply(shape)\n",
    "\n",
    "# Add a field for the image centroid\n",
    "img_df['img_centroid'] = img_df['img_geom'].iloc[0].centroid\n",
    "\n",
    "# Convert DataFrame to GeoDataFrame\n",
    "img_gdf = gpd.GeoDataFrame(img_df, geometry='img_geom', crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_gdf.shape)\n",
    "img_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Images with Polygons\n",
    "Inputs:\n",
    "- poly_gdf: geodataframe of polygon metadata\n",
    "- img_gdf: geodataframe of maxar image metadata\n",
    "\n",
    "Outputs:\n",
    "- merged: merged geodataframe of maxar image metadata + associated polygon metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the image data with the polygon data (preserving image data rows and adding associated polygon attributes)\n",
    "merged_gdf = img_gdf.merge(poly_gdf, on=['project_id', 'poly_id'], how='left')\n",
    "\n",
    "# Ensure correct datetime format\n",
    "merged_gdf['plantstart'] = pd.to_datetime(merged_gdf['plantstart'], errors='coerce')\n",
    "merged_gdf['img_date'] = pd.to_datetime(merged_gdf['img_date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_gdf.shape)\n",
    "print(merged_gdf.columns)\n",
    "merged_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the number of images by project and polygon for merged (unfiltered) images\n",
    "merged_gdf_summary = (\n",
    "    merged_gdf.groupby(['project_id', 'poly_id'])\n",
    "    .size()\n",
    "    .reset_index(name='merged_img_count')\n",
    ")\n",
    "\n",
    "merged_gdf_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Images Based on Constraints\n",
    "Inputs:\n",
    "- merged: merged dataframe of maxar image metadata + associated polygon metadata\n",
    "\n",
    "Outputs:\n",
    "- filtered_merged: a filtered version of the merged dataframe of maxar image metadata + associated polygon metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a date differential column\n",
    "merged_gdf['date_diff'] = (merged_gdf['img_date'] - merged_gdf['plantstart']).dt.days\n",
    "\n",
    "# Filter to retain only images within the desired time range, cloud cover, off nadir angle, and sun elevation parameters\n",
    "img_gdf_filtered = merged_gdf[\n",
    "    (merged_gdf['date_diff'] >= baseline_range[0]) &\n",
    "    (merged_gdf['date_diff'] <= baseline_range[1]) &\n",
    "    (merged_gdf['area:cloud_cover_percentage'] < cloud_thresh) &\n",
    "    (merged_gdf['area:avg_off_nadir_angle'] <= off_nadir_thresh) &\n",
    "    (merged_gdf['view:sun_elevation'] >= sun_elev_thresh)\n",
    "].copy()    # Copy to avoid SettingWithCopyWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('img_gdf_filtered Unique Polygons:', len(img_gdf_filtered['poly_id'].unique()))\n",
    "img_gdf_filtered['poly_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize  the number of images by project and polygon for filtered (by date and cloud cover) images\n",
    "img_gdf_filtered_summary = (\n",
    "    img_gdf_filtered.groupby(['project_id', 'poly_id'])\n",
    "    .size()\n",
    "    .reset_index(name='filtered_img_count')\n",
    ")\n",
    "\n",
    "img_gdf_filtered_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the # of images with different cloud cover percentages in the filtered imagery\n",
    "print(img_gdf_filtered.shape)\n",
    "img_gdf_filtered['area:cloud_cover_percentage'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Coverage for Each Polygon\n",
    "Input:\n",
    "- poly_gdf: geodataframe of polygon metadata\n",
    "- img_gdf_filtered: a filtered version of the merged geodataframe of maxar image metadata + associated polygon metadata\n",
    "\n",
    "Output:\n",
    "- csv of percent imagery coverage by project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRATCH IGNORE\n",
    "#  Empty list to hold results\n",
    "results = []\n",
    "\n",
    "for _, polygon in poly_gdf.iterrows():\n",
    "    poly_id = polygon['poly_id']\n",
    "    project_id = polygon['project_id']\n",
    "    poly_geom = polygon['poly_geom'] # Geometry column\n",
    "    poly_area = polygon['calcarea']  # Precomputed area of the polygon in hectares\n",
    "\n",
    "    # Create a filtered GeoDataFrame that contains only images that intersect the polygon\n",
    "    images = img_gdf_filtered[img_gdf_filtered[\"img_geom\"].intersects(poly_geom)]\n",
    "\n",
    "images\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRATCH IGNORE\n",
    "# Create empty dictionary to store poly_ids for each prj_id\n",
    "project_polygons = {}\n",
    "\n",
    "# Extract a list of the unique project_id values from shp_gdf   \n",
    "prj_keys = list(set(poly_gdf.project_id))\n",
    "\n",
    "for key in prj_keys:\n",
    "    print('project key:', key)\n",
    "    # Extract all poly_ids associated with this project_id\n",
    "    poly_keys = poly_gdf[poly_gdf['project_id'] == key]['poly_id'].tolist()\n",
    "    #print('poly keys:', poly_keys)\n",
    "\n",
    "    # Store it in a dictionary\n",
    "    project_polygons[key] = poly_keys\n",
    "\n",
    "# Print to check\n",
    "for key, polys in list(project_polygons.items()):\n",
    "    print(f\"Project ID: {key}, Polygon IDs: {polys}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRATCH IGNORE\n",
    "for project_id, poly_list in project_polygons.items():\n",
    "    print(project_id, poly_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized option to select a best image per polygon from Rhiannon's MSU code. But can't add complex logic like combining two images in it\n",
    "mins_metadata = img_gdf_filtered.loc[img_gdf_filtered.groupby('poly_id')['area:cloud_cover_percentage'].idxmin()].reset_index(drop=True)\n",
    "mins_metadata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproject Polygons & Image Footprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_utm_crs(long, lat):\n",
    "    \"\"\"\n",
    "    Determine the best UTM CRS based on polygon centroid location\n",
    "    \"\"\"\n",
    "    utm_zone = int((long + 180) / 6) + 1\n",
    "    hemisphere = 32600 if lat >= 0 else 32700 # Northern vs Southern hemisphere\n",
    "    return f\"EPSG:{hemisphere+utm_zone}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a dictionary mapping project_id --> list of poly_ids\n",
    "project_polygons = poly_gdf.groupby(\"project_id\")[\"poly_id\"].apply(list).to_dict()\n",
    "\n",
    "# Create an empty list to store\n",
    "results = []\n",
    "\n",
    "# Step 2: Iterate through each project_id\n",
    "for project_id, poly_list in project_polygons.items():\n",
    "    # First, filter img_gdf_filtered by 'project_id'\n",
    "    project_images = img_gdf_filtered[img_gdf_filtered['project_id'] == project_id]\n",
    "    print(f\"There are {len(project_images)} images in the filtered image dataset associated with Project: {project_id}\")\n",
    "\n",
    "    # Step 3: Iterate through each polygon in this project\n",
    "    for poly_id in poly_list:\n",
    "        print(f\"Checking polygon {poly_id}...\")\n",
    "        # Retrieve polygon geometry\n",
    "        polygon_row = poly_gdf[poly_gdf['poly_id'] == poly_id].iloc[0]\n",
    "        poly_geom = polygon_row[\"poly_geom\"]\n",
    "        poly_area = polygon_row[\"calcarea\"] # For now, using precomputed area (in hectares)\n",
    "        print(f\"Polygon {poly_id}'s area is {poly_area} hectares\")\n",
    "\n",
    "        # Get the centroid of the polygon to determine the UTM zone\n",
    "        #poly_centroid = poly_geom.centroid\n",
    "        #print(f\"Polygon Centroid for Polygon {poly_id} is {poly_centroid}\")\n",
    "        utm_crs = get_utm_crs(poly_centroid.x, poly_centroid.y) # Determine the best UTM CRS\n",
    "        print('UTM CRS:', utm_crs)\n",
    "\n",
    "        # Reproject the polygon to its correct UTM Zone\n",
    "        poly_reprojected = gpd.GeoDataFrame([polygon_row], geometry=[poly_geom], crs=\"EPSG:4326\").to_crs(utm_crs)\n",
    "        poly_geom_reprojected = poly_reprojected.geometry.iloc[0] # Extract reprojected polygon\n",
    "\n",
    "        # Now filter 'project_images' by 'poly_id'\n",
    "        poly_images = project_images[project_images['poly_id'] == poly_id]\n",
    "        print(f\"Before reprojecting, there are {len(poly_images)} images in the filtered image dataset for Polygon {poly_id}\")\n",
    "        print(f\"Original CRS: {poly_images.crs}\")\n",
    "\n",
    "        # If there are no images for this polygon, append that to the results list\n",
    "        if poly_images.empty:\n",
    "            # No imagery --> 0% coverage\n",
    "            results.append((poly_id, project_id, poly_area, 0, 0, 0))\n",
    "            continue\n",
    "\n",
    "        # Reproject the images to the same UTM CRS\n",
    "        #poly_images_reprojected = gpd.GeoDataFrame(poly_images, geometry=poly_images['img_geom'], crs='EPSG:4326').to_crs(utm_crs)\n",
    "        # Remove invalid or empty geometries\n",
    "        poly_images = poly_images[poly_images.is_valid]\n",
    "        poly_images = poly_images[~poly_images.is_empty]\n",
    "\n",
    "        # Reset index to prevent row loss\n",
    "        poly_images = poly_images.reset_index(drop=True)        \n",
    "        poly_images = poly_images.to_crs(utm_crs)\n",
    "        print(f\"After reprojecting, there are {len(images_reprojected)} reprojected images for Polygon {poly_id}\")\n",
    "        print(f\"Reprojected CRS: {poly_images.crs}\")\n",
    "\n",
    "        # Step 4: Select best image(s) (lowest cloud cover OR union all images)\n",
    "        sorted_images = poly_images_reprojected.sort_values(by='area:cloud_cover_percentage') # Sort by lowest cloud cover percentage\n",
    "        best_image = sorted_images.iloc[0] # Take the 1 best image (for now)\n",
    "        best_image_geom = best_image.geometry\n",
    "        print(f\"The best image for Polygon {poly_id} is {best_image['title']}\")\n",
    "\n",
    "        # Step 5: Compute actual coverage of polygon using the best image\n",
    "        covered_area = best_image_geom.intersection(poly_geom_reprojected).area # In square meters (really?)\n",
    "        convered_area_ha = covered_area / 10000 # Convert to hectares\n",
    "        print(f\"The calculated covered area for polygon {poly_id} is {convered_area_ha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
