{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maxar Image Availability Analysis\n",
    "\n",
    "The Maxar image availability workflow takes as input a list of TerraFund project ids and returns as output a csv listing every project and how much of that project’s area has Maxar imagery coverage.\n",
    "\n",
    "#### Workflow:\n",
    "1. Pull info on project characteristics for the entire portfolio using the TerraMatch API\n",
    "    - Repo/notebook: terrafund-portfolio-analysis/tm-api.ipynb\n",
    "    - Input: list of TerraFund project IDs\n",
    "    - Output: csv of all project features\n",
    "2. Using the TM API csv, pull Maxar metadata\n",
    "    - Repo/notebook: maxar-tools/decision-tree-metadata.ipynb and maxar-tools/src/decision_tree.py (? may need to change b/c of my additions to the acquire_metadata function)\n",
    "    - Input: csv of project features\n",
    "    - Output: csv of maxar metadata\n",
    "3. Create imagery features (??)\n",
    "    - Repo/notebook: terrafund-portfolio-analysis/maxar-img-avail.py\n",
    "    - Input: csv of maxar metadata and csv of TM project features\n",
    "    - Output: csv of project features and percent imagery coverage\n",
    "4. Identify projects with 100% imagery coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # used\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import shape\n",
    "from shapely.geometry import Polygon, Point\n",
    "from shapely import union_all\n",
    "import ast\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import requests\n",
    "import yaml\n",
    "import json\n",
    "import pyproj\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "import image_availability as img\n",
    "import process_api_results as clean\n",
    "import decision_trees as tree\n",
    "import tm_api_utils as api_request\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "tm_auth_path = '../secrets.yaml'\n",
    "tm_staging_url = \"https://api-staging.terramatch.org/research/v3/sitePolygons?\"                 # use for testing queries\n",
    "tm_prod_url = \"https://api.terramatch.org/research/v3/sitePolygons?\"                            # Use to pull data for analysis'\n",
    "approved_projects = '../terrafund-portfolio-analyses/projects_all_approved_202501091214.csv'    # List of projects with approved polygons\n",
    "feats = '../data/tm_api_TEST.csv'                                                               # Polygon metadata & geometries from TM API\n",
    "maxar_feats = '/home/darby/github_repos/maxar-tools/data/tm_api_TEST.csv'                       # Polygon metadata & geometries from TM API saved to maxar-tools repo\n",
    "maxar_md = '../data/imagery_availability/comb_img_availability_2025-02-26.csv'                  # Metadata for Maxar images corresponding to polygons\n",
    "\n",
    "# Define filtering thesholds (stored in a dictionary)\n",
    "filters = {\n",
    "    'cloud_cover': 50,          # Remove images with >50% cloud cover\n",
    "    'off_nadir': 30,            # Remove images with >30° off-nadir angle\n",
    "    'sun_elevation': 30,        # Keep only images where sun elevation >30°\n",
    "    'date_range': (-366, 0),    # Date range of 1 year before plantstart\n",
    "    'img_count': 1,             # Threshold for identifying image availability (REASSESS)\n",
    "    'ev_range': (730,1095)      # Early verification window (2-3 years after plantstart date) (REASSESS)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Workflow Outline (DON'T RUN!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: LOAD AND PREPROCESS DATA\n",
    "# 1.1: Load polygon dataset\n",
    "poly_csv = gpd.GeoDataFrame(polygon geometries & metadata)\n",
    "\n",
    "# 1.2 Load image dataset\n",
    "img_csv = gpd.GeoDataFrame(maxar image geometries & metadata)\n",
    "\n",
    "# 1.3 Preprocess the data\n",
    "poly_gdf = preprocess_polygons(poly_csv) # Clean data, convert geometries, enforce CRS\n",
    "img_gdf = preprocess_images(img_csv) # Clean data, convert geometries, enforce CRS\n",
    "\n",
    "\n",
    "# Step 2: MERGE POLYGON DATA WITH IMAGE DATA\n",
    "merged_gdf = img_gdf.merge(poly_gdf, on=['project_id', 'poly_id'], how='left')\n",
    "\n",
    "# Step 3: PRE-FILTER IMAGES\n",
    "filtered_images = merged_gdf where:\n",
    "    (date is within allowed date range) &\n",
    "    (cloud cover < cloud_thresh) &\n",
    "    (off-nadir angle < off_nadir_thresh) &\n",
    "    (sun elevation < sun_elev_thresh)\n",
    "\n",
    "# Step 4: ITERATE THROUGH PROJECTS AND POLYGONS TO CALCULATE IMAGERY COVERAGE\n",
    "# 4.1 Create a dictionary for project-polygon mapping\n",
    "project_polygons = {project_id: list of poly_ids associated with that project} # Create a dictionary\n",
    "\n",
    "# 4.2 Initialize list to store low coverage cases\n",
    "low_img_coverage_log = []\n",
    "\n",
    "# 4.3 Iterate through each project\n",
    "for each project_id in project_polygons:\n",
    "\n",
    "    # 4.4 Get all polygons for this project\n",
    "    project_polygons_list = list of poly_ids for this project_id\n",
    "\n",
    "    # 4.5 Iterate through each polygon in the project\n",
    "    for each poly_id in project_polygons_list:\n",
    "    \n",
    "        # 4.6 Get all images associated with this polygon\n",
    "        poly_images = filtered_images[filtered_images['poly_id'] == poly_id]\n",
    "\n",
    "        # Count the number of available images\n",
    "        num_images = len(poly_images)\n",
    "\n",
    "        # If no valid image exists, record 0% coverage\n",
    "        if poly_images is empty:\n",
    "            store result: (poly_id, project_id, None, num_images, 0, 0) # No images available\n",
    "            continue\n",
    "\n",
    "        # 4.7 Select the best image (lowest cloud cover)\n",
    "        best_image = select_best_image(poly_images)\n",
    "\n",
    "        # 4.8 Get polygon and image geometries\n",
    "        poly_geom = poly_gdf[poly_gdf['poly_id'] == poly_id].geometry.iloc[0]\n",
    "        best_img_geom = best_image['img_geom']\n",
    "\n",
    "        # 4.9 Compute UTM Zone and reproject geometries\n",
    "        poly_centroid = compute centroid of poly_geom\n",
    "        utm_crs = get UTM CRS from centroid\n",
    "        poly_geom_reprojected = reproject poly_geom to utm_crs\n",
    "        best_img_geom_reprojected = reproject best_img_geom to utm_crs\n",
    "\n",
    "        # 4.10 Calculate the polygon area dynamically (in hectares)\n",
    "        poly_area_ha = poly_geom_reprojected.area / 10000\n",
    "\n",
    "        # 4.11 Calculate area of overlap\n",
    "        overlap_area = poly_geom_reprojected union best_img_geom_reprojected\n",
    "        overlap_area_ha = overlap_area / 10000\n",
    "\n",
    "        # 4.12 Compute percent of polygon area covered\n",
    "        percent_img_cover = (overlap_area / poly_area_ha) * 100\n",
    "\n",
    "        # 4.13 Log cases where imagery coverage is unexpectedly low\n",
    "        if percent_img_cover < 50:\n",
    "            log_entry = {\n",
    "                'poly_id': poly_id,\n",
    "                'project_id': project_id,\n",
    "                'best_image': best_image['title'],\n",
    "                'num_images': num_images,\n",
    "                'poly_area_ha': poly_area_ha,\n",
    "                'overlap_area_ha': overlap_area_ha,\n",
    "                'percent_img_cover': percent_img_cover\n",
    "            }\n",
    "            low_img_coverage_log.append(log_entry)\n",
    "\n",
    "        # 4.14 Store results\n",
    "        store result: (poly_id, project_id, best_image['title'], num_images, poly_area_ha, overlap_area_ha, percent_img_cover)\n",
    "\n",
    "# STEP 5: EXPORT LOW COVERAGE LOG IF NEEDED\n",
    "if low_img_coverage_log is not empty:\n",
    "    export_to_csv(low_img_coverage_log, \"low_coverage_polygons.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: LOAD & PREPROCESS DATA\n",
    "Goal: ensure input data is clean & structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.1 LOAD IN POLYGON AND IMAGE CSVS\n",
    "poly_df = pd.read_csv(feats)\n",
    "img_df = pd.read_csv(maxar_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.2 PREPROCESS POLYGON DATA\n",
    "def preprocess_polygons(poly_df, debug=False):\n",
    "    \"\"\"\n",
    "    Cleans up a dataframe of polygon metadata & geometries from the TerraMatch API and \n",
    "    converts it into a GeoDataframe\n",
    "\n",
    "    Args:\n",
    "        poly_df (DataFrame): Raw polygon dataset.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: Processed polygon dataset with a geometry column as a shapely object.\n",
    "    \"\"\"\n",
    "    # Enforce lowercase column names\n",
    "    poly_df.columns = poly_df.columns.str.lower()\n",
    "\n",
    "    # Rename 'name' and 'geometry' columns\n",
    "    poly_df = poly_df.rename(columns={'name': 'poly_name', 'geometry': 'poly_geom'})\n",
    "\n",
    "    # Convert 'plantstart' column to a datetime\n",
    "    poly_df['plantstart'] = pd.to_datetime(poly_df['plantstart'], errors='coerce')\n",
    "\n",
    "    # Convert stringified 'poly_geom' dictionaries into real dictionaries\n",
    "    poly_df['poly_geom'] = poly_df['poly_geom'].apply(lambda x: shape(ast.literal_eval(x)) if isinstance(x, str) else shape(x))\n",
    "\n",
    "    # Convert 'poly_geom' dictionaries from WKT to Shapely objects\n",
    "    poly_df['poly_geom'] = poly_df['poly_geom'].apply(shape)\n",
    "\n",
    "    # Convert to GeoDataFrame\n",
    "    poly_gdf = gpd.GeoDataFrame(poly_df, geometry='poly_geom', crs=\"EPSG:4326\")\n",
    "\n",
    "    # Add a field for the polygon centroid\n",
    "    poly_gdf['poly_centroid'] = poly_gdf['poly_geom'].iloc[0].centroid\n",
    "\n",
    "    if debug:\n",
    "        print(f\"There are {len(poly_gdf.poly_id.unique())} unique polygons for {len(poly_gdf.project_id.unique())} projects in this dataset.\")\n",
    "\n",
    "    return poly_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.3 PREPROCESS MAXAR IMAGERY DATA\n",
    "def preprocess_images(img_df, debug=True):\n",
    "    \"\"\"\n",
    "    Cleans up a dataframe of maxar image metadata & geometries from the Maxar Discovery API and \n",
    "    converts it into a GeoDataframe\n",
    "\n",
    "    Args:\n",
    "        img_df (DataFrame): Raw image metadata dataset.\n",
    "    \n",
    "    Returns: \n",
    "        GeoDataFrame: Processed image dataset with a geometry column as a shapely object.\n",
    "    \"\"\"\n",
    "    # Convert 'datetime' column to a datetime and rename\n",
    "    img_df.loc[:, 'datetime'] = pd.to_datetime(img_df['datetime'], format='%Y-%m-%dT%H:%M:%S.%fZ', errors='coerce') # Convert to datetime type\n",
    "    img_df.loc[:, 'datetime'] = img_df['datetime'].apply(lambda x: x.replace(tzinfo=None) if pd.notna(x) else x)    # Remove time zone info\n",
    "    \n",
    "    # Rename the 'datetime' column to 'img_date'\n",
    "    img_df = img_df.rename(columns={'datetime': 'img_date'}) # Rename the column img_date\n",
    "\n",
    "    # Select the relevent columns from img_df\n",
    "    img_df = img_df[['title', 'project_id', 'poly_id', 'img_date', 'area:cloud_cover_percentage', 'eo:cloud_cover', 'area:avg_off_nadir_angle', 'view:sun_elevation', 'img_geom']]\n",
    "\n",
    "    # Convert stringified 'poly_geom' dictionaries into real dictionaries\n",
    "    img_df['img_geom'] = img_df['img_geom'].apply(lambda x: shape(ast.literal_eval(x)) if isinstance(x, str) else shape(x))\n",
    "\n",
    "    # Convert 'img_geom' (image footprint geometries) from WKT to Shapely objects\n",
    "    img_df['img_geom'] = img_df['img_geom'].apply(shape)\n",
    "\n",
    "    # Convert DataFrame to GeoDataFrame\n",
    "    img_gdf = gpd.GeoDataFrame(img_df, geometry='img_geom', crs=\"EPSG:4326\")\n",
    "\n",
    "    # Add a field for the image centroid\n",
    "    img_gdf['img_centroid'] = img_gdf['img_geom'].iloc[0].centroid\n",
    "\n",
    "    if debug:\n",
    "        print(f\"There are {len(img_gdf)} images for {len(img_gdf.poly_id.unique())} polygons in {len(img_gdf.project_id.unique())} projects in this dataset.\")\n",
    "\n",
    "    return img_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: MERGE & FILTER DATA\n",
    "Goal: link images to polygons and apply filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.1 MERGE THE POLYGON ATTRIBUTES TO THE IMAGES GEODATAFRAME\n",
    "def merge_polygons_images(img_gdf, poly_gdf, debug=True):\n",
    "    \"\"\" \n",
    "    Merges the polygon metadata into the Maxar image GeoDataFrame. All rows of the img_gdf are preserved.\n",
    "    Also records polygons that are dropped because they don't have any associated images.\n",
    "\n",
    "    Args:\n",
    "        img_gdf (GeoDataFrame): Image metadata dataset (each row represents a Maxar image)\n",
    "        poly_gdf (GeoDataFrame): Polygon dataset (each row represents a polygon from the TM API)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (GeoDataFrame of merged dataset, list of missing polygons (poly_id, project_id))\n",
    "    \"\"\"\n",
    "    # Merge the image data with the polygon data (preserving image data rows and adding associated polygon attributes)\n",
    "    merged_gdf = img_gdf.merge(poly_gdf, on=['project_id', 'poly_id'], how='left')\n",
    "\n",
    "    # Identify polygons without any corresponding Maxar images\n",
    "    missing_polygons_df = poly_gdf[~poly_gdf['poly_id'].isin(merged_gdf['poly_id'])]\n",
    "\n",
    "    # Save poly_id and project_id of missing polygons as a list of tuples\n",
    "    missing_polygons_list = list(missing_polygons_df[['poly_id', 'project_id']].itertuples(index=False, name=None))\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Total images in img_gdf: {len(img_gdf)}\")\n",
    "        print(f\"Total polygons in poly_gdf: {len(poly_gdf)}\")\n",
    "        print(f\"Total rows in merged dataset: {len(merged_gdf)}\")\n",
    "        print(f\"Unique polygons in merged dataset: {len(merged_gdf['poly_id'].unique())}\")\n",
    "    \n",
    "        # Count polygons dropped due to no matching images\n",
    "        missing_polygons = len(poly_gdf[~poly_gdf['poly_id'].isin(merged_gdf['poly_id'])])\n",
    "        print(f\"There {missing_polygons} polygons without images in the merged dataset\")\n",
    "        print(f\"Polygons without images (dropped at this stage): {missing_polygons_list}\")\n",
    "\n",
    "    return merged_gdf, missing_polygons_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.2 FILTER IMAGES BASED ON HARD CRITERIA\n",
    "def filter_images(merged_gd, filters, debug=True):\n",
    "    \"\"\"\n",
    "    Filters the merged dataset to retain only images that meet filters for image quality.\n",
    "    The values for the filters can be changed in the parameters section.\n",
    "\n",
    "    Args:\n",
    "        merged_gdf (GeoDataFrame): Merged dataset of images and polygons.\n",
    "        filters (dict): Dictionary containing filter thresholds (in Parameters section of notebook)\n",
    "    \n",
    "    Returns:\n",
    "        GeoDataFrame: Filtered dataset containing only the images that meet the criteria\n",
    "    \"\"\"\n",
    "    # Ensure date columns are in correct datetime format\n",
    "    merged_gdf['img_date'] = pd.to_datetime(merged_gdf['img_date'], errors='coerce')\n",
    "    merged_gdf['plantstart'] = pd.to_datetime(merged_gdf['plantstart'], errors='coerce')\n",
    "\n",
    "    # Compute the date difference (image capture date - plant start date)\n",
    "    merged_gdf['date_diff'] = (merged_gdf['img_date'] - merged_gdf['plantstart']).dt.days\n",
    "\n",
    "    # Apply filtering criteria to retain only images within the desired time range, cloud cover, \n",
    "    # off nadir angle, and sun elevation parameters\n",
    "    filtered_images = merged_gdf[\n",
    "        (merged_gdf['date_diff'] >= filters['date_range'][0]) &\n",
    "        (merged_gdf['date_diff'] <= filters['date_range'][1]) &\n",
    "        (merged_gdf['area:cloud_cover_percentage'] < filters['cloud_cover']) &\n",
    "        (merged_gdf['area:avg_off_nadir_angle'] <= filters['off_nadir']) &\n",
    "        (merged_gdf['view:sun_elevation'] >= filters['sun_elevation'])\n",
    "    ].copy()  # Copy to avoid SettingWithCopyWarning\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Total images before filtering: {len(merged_gdf)}\")\n",
    "        print(f\"Total images after filtering: {len(filtered_images)}\")\n",
    "        print(f\"Polygons with at least one valid image: {filtered_images['poly_id'].nunique()}\")\n",
    "    \n",
    "    return filtered_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3: PROCESS EACH POLYGON\n",
    "Goal: Prepare polygons & select best image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.1 GET THE BEST IMAGE FOR A GIVEN POLYGON\n",
    "def get_best_image(poly_images, debug=True):\n",
    "    \"\"\"\n",
    "    Selects the best image for a given polygon based on the lowest cloud cover. If multiple images have\n",
    "    the same cloud cover, selects the one closest to the plantstart date. \n",
    "\n",
    "    If we want to update this to include an \"expected coverage\" based on cloud cover and footprint overlap,\n",
    "    this is where it would go.\n",
    "\n",
    "    Args:\n",
    "        poly_images (GeoDataFrame): Subset of img_gdf_filtered containing images for one polygon.\n",
    "    \n",
    "    Returns:\n",
    "        GeoSeries: The best image row from poly_images\n",
    "    \"\"\"\n",
    "    # Create an absolute value date_diff column to help sort images by proximity to plantstart date\n",
    "    poly_images = poly_images.copy() # Avoid modifying the original dataframe\n",
    "    poly_images['abs_date_diff'] = poly_images['date_diff'].abs()\n",
    "\n",
    "    # Sort images by cloud cover (ascending) and then by date (closest to plantstart)\n",
    "    sorted_images = poly_images.sort_values(by=['area:cloud_cover_percentage', 'abs_date_diff'])\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\n Debug: Sorted images for this polygon (using cloud cover, then proximity to plantstart date):\")\n",
    "        print(sorted_images[['title', 'area:cloud_cover_percentage', 'img_date', 'plantstart', 'abs_date_diff']])\n",
    "\n",
    "    # Select the best image (first row after sorting)\n",
    "    best_image = sorted_images.iloc[0]\n",
    "\n",
    "    return best_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4: COMPUTE COVERAGE\n",
    "Goal: calculate imagery coverage per polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 5: EXPORT RESULTS\n",
    "Goal: save results for review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.1 LOAD IN POLYGON AND IMAGE CSVS\n",
    "poly_df = pd.read_csv(feats)\n",
    "img_df = pd.read_csv(maxar_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16 unique polygons for 3 projects in this dataset.\n"
     ]
    }
   ],
   "source": [
    "## 1.2 PREPROCESS POLYGON DATA\n",
    "poly_gdf = preprocess_polygons(poly_df, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 229 images for 16 polygons in 3 projects in this dataset.\n"
     ]
    }
   ],
   "source": [
    "## 1.2 PREPROCESS POLYGON DATA\n",
    "img_gdf = preprocess_images(img_df, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in img_gdf: 229\n",
      "Total polygons in poly_gdf: 16\n",
      "Total rows in merged dataset: 229\n",
      "Unique polygons in merged dataset: 16\n",
      "There 0 polygons without images in the merged dataset\n",
      "Polygons without images (dropped at this stage): []\n"
     ]
    }
   ],
   "source": [
    "## 2.1 MERGE THE POLYGON ATTRIBUTES TO THE IMAGES GEODATAFRAME\n",
    "merged_gdf, missing_polygons = merge_polygons_images(img_gdf, poly_gdf, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images before filtering: 229\n",
      "Total images after filtering: 30\n",
      "Polygons with at least one valid image: 15\n"
     ]
    }
   ],
   "source": [
    "### 3.1 FILTER IMAGES BASED ON HARD CRITERIA\n",
    "img_gdf_filtered = filter_images(merged_gdf, filters, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'project_id', 'poly_id', 'img_date',\n",
      "       'area:cloud_cover_percentage', 'eo:cloud_cover',\n",
      "       'area:avg_off_nadir_angle', 'view:sun_elevation', 'img_geom',\n",
      "       'img_centroid', 'poly_name', 'status', 'siteid', 'poly_geom',\n",
      "       'plantstart', 'plantend', 'practice', 'targetsys', 'distr', 'numtrees',\n",
      "       'calcarea', 'indicators', 'establishmenttreespecies',\n",
      "       'reportingperiods', 'poly_centroid', 'date_diff'],\n",
      "      dtype='object')\n",
      "\n",
      " Debug: Sorted images for this polygon (using cloud cover, then proximity to plantstart date):\n",
      "                                title  area:cloud_cover_percentage  \\\n",
      "13  Maxar WV03 Image 1040010068D32900                          0.0   \n",
      "16  Maxar WV02 Image 10300100B3A8FB00                          0.0   \n",
      "\n",
      "                     img_date plantstart  abs_date_diff  \n",
      "13 2021-06-13 07:40:21.255153 2022-01-09            210  \n",
      "16 2021-01-17 07:35:47.871523 2022-01-09            357  \n"
     ]
    }
   ],
   "source": [
    "# Manually get images for a polygon\n",
    "example_poly_images = img_gdf_filtered[img_gdf_filtered['poly_id'] == '410696dc-9579-4412-9c7b-55194cb1867c']\n",
    "best_image = get_best_image(example_poly_images, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                             Maxar WV03 Image 1040010068D32900\n",
       "project_id                     3a860077-df4c-4e95-8fec-41520c551243\n",
       "poly_id                        410696dc-9579-4412-9c7b-55194cb1867c\n",
       "img_date                                 2021-06-13 07:40:21.255153\n",
       "plantstart                                      2022-01-09 00:00:00\n",
       "area:cloud_cover_percentage                                     0.0\n",
       "area:avg_off_nadir_angle                                   25.09837\n",
       "view:sun_elevation                                        52.087266\n",
       "Name: 13, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_image[['title', 'project_id', 'poly_id', 'img_date', 'plantstart', 'area:cloud_cover_percentage',\n",
    "            'area:avg_off_nadir_angle', 'view:sun_elevation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
