{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maxar Image Availability Analysis\n",
    "\n",
    "The Maxar image availability workflow takes as input a list of TerraFund project ids and returns as output a csv listing every project and how much of that projectâ€™s area has Maxar imagery coverage.\n",
    "\n",
    "#### Workflow:\n",
    "1. Pull info on project characteristics for the entire portfolio using the TerraMatch API\n",
    "    - Repo/notebook: terrafund-portfolio-analysis/tm-api.ipynb\n",
    "    - Input: list of TerraFund project IDs\n",
    "    - Output: csv of all project features\n",
    "2. Using the TM API csv, pull Maxar metadata\n",
    "    - Repo/notebook: maxar-tools/decision-tree-metadata.ipynb and maxar-tools/src/decision_tree.py (? may need to change b/c of my additions to the acquire_metadata function)\n",
    "    - Input: csv of project features\n",
    "    - Output: csv of maxar metadata\n",
    "3. Create imagery features (??)\n",
    "    - Repo/notebook: terrafund-portfolio-analysis/maxar-img-avail.py\n",
    "    - Input: csv of maxar metadata and csv of TM project features\n",
    "    - Output: csv of project features and percent imagery coverage\n",
    "4. Identify projects with 100% imagery coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import shape\n",
    "from shapely.geometry import Polygon, Point\n",
    "from shapely import union_all\n",
    "import ast\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import requests\n",
    "import yaml\n",
    "import json\n",
    "import pyproj\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "import image_availability as img\n",
    "import process_api_results as clean\n",
    "import decision_trees as tree\n",
    "import tm_api_utils as api_request\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "tm_auth_path = '../secrets.yaml'\n",
    "tm_staging_url = \"https://api-staging.terramatch.org/research/v3/sitePolygons?\"                 # use for testing queries\n",
    "tm_prod_url = \"https://api.terramatch.org/research/v3/sitePolygons?\"                            # Use to pull data for analysis'\n",
    "approved_projects = '../terrafund-portfolio-analyses/projects_all_approved_202501091214.csv'    # List of projects with approved polygons\n",
    "feats = '../data/tm_api_TEST.csv'                                                               # Polygon metadata & geometries from TM API\n",
    "maxar_feats = '/home/darby/github_repos/maxar-tools/data/tm_api_TEST.csv'                       # Polygon metadata & geometries from TM API saved to maxar-tools repo\n",
    "maxar_md = '../data/imagery_availability/comb_img_availability_2025-02-26.csv'                  # Metadata for Maxar images corresponding to polygons\n",
    "\n",
    "# Define thesholds\n",
    "cloud_thresh = 50             # Threshold for removing cloudy imagery\n",
    "off_nadir_thresh = 30         # Threshold for removing imagery too far off nadir\n",
    "sun_elev_thresh = 30          # Threshold for removing imagery with too steep of a sun angle\n",
    "img_count = 1                 # Threshold for identifying image availability\n",
    "baseline_range = (-366, 0)    # Baseline window (1 year before plantstart date)\n",
    "ev_range = (730, 1095)        # Early verification window (2-3 years after plant start date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load & Preprocess Data\n",
    "Inputs: \n",
    "- TM API csv\n",
    "- Maxar metadata csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TM API polygons and convert to a GeoDataFrame\n",
    "polygons = pd.read_csv(feats)\n",
    "polygons.columns = polygons.columns.str.lower()   # Enforce lowercase column names\n",
    "\n",
    "# Rename 'name' and 'geometry' columns\n",
    "poly_df = polygons.rename(columns={'name': 'poly_name', 'geometry': 'poly_geom'})  \n",
    "\n",
    "# Convert 'plantstart' column to a datetime\n",
    "poly_df['plantstart'] = pd.to_datetime(poly_df['plantstart'], errors='coerce')\n",
    "\n",
    "# Convert stringified 'poly_geom' dictionaries into real dictionaries\n",
    "poly_df['poly_geom'] = poly_df['poly_geom'].apply(lambda x: shape(ast.literal_eval(x)) if isinstance(x, str) else shape(x))\n",
    "\n",
    "# Convert 'poly_geom' (polygon geometries) from WKT to Shapely objects\n",
    "poly_df['poly_geom'] = poly_df['poly_geom'].apply(shape)\n",
    "\n",
    "# Add a field for the polygon centroid\n",
    "poly_df['poly_centroid'] = poly_df['poly_geom'].iloc[0].centroid\n",
    "\n",
    "# Convert DataFrame to GeoDataFrame\n",
    "poly_gdf = gpd.GeoDataFrame(poly_df, geometry='poly_geom', crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Maxar images metadata and convert to a GeoDataFrame\n",
    "images = pd.read_csv(maxar_md)\n",
    "\n",
    "# Select relevent columns\n",
    "img_df = images[['title', 'project_id', 'poly_id', 'datetime', 'area:cloud_cover_percentage', 'eo:cloud_cover', 'area:avg_off_nadir_angle', 'view:sun_elevation', 'img_geom']]\n",
    "\n",
    "# Convert 'datetime' column to a datetime and rename\n",
    "img_df.loc[:, 'datetime'] = pd.to_datetime(img_df['datetime'], format='%Y-%m-%dT%H:%M:%S.%fZ', errors='coerce') # Convert to datetime type\n",
    "img_df.loc[:, 'datetime'] = img_df['datetime'].apply(lambda x: x.replace(tzinfo=None) if pd.notna(x) else x)    # Remove time zone info\n",
    "img_df = img_df.rename(columns={'datetime': 'img_date'})                                                        # Rename 'datetime' column 'img_date'\n",
    "\n",
    "# Convert stringified 'poly_geom' dictionaries into real dictionaries\n",
    "img_df['img_geom'] = img_df['img_geom'].apply(lambda x: shape(ast.literal_eval(x)) if isinstance(x, str) else shape(x))\n",
    "\n",
    "# Convert 'img_geom' (image footprint geometries) from WKT to Shapely objects\n",
    "img_df['img_geom'] = img_df['img_geom'].apply(shape)\n",
    "\n",
    "# Add a field for the image centroid\n",
    "img_df['img_centroid'] = img_df['img_geom'].iloc[0].centroid\n",
    "\n",
    "# Convert DataFrame to GeoDataFrame\n",
    "img_gdf = gpd.GeoDataFrame(img_df, geometry='img_geom', crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Merge Images with Polygons\n",
    "Inputs:\n",
    "- poly_gdf: geodataframe of polygon metadata\n",
    "- img_gdf: geodataframe of maxar image metadata\n",
    "\n",
    "Outputs:\n",
    "- merged: merged geodataframe of maxar image metadata + associated polygon metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the image data with the polygon data (preserving image data rows and adding associated polygon attributes)\n",
    "merged_gdf = img_gdf.merge(poly_gdf, on=['project_id', 'poly_id'], how='left')\n",
    "\n",
    "# Ensure correct datetime format\n",
    "merged_gdf['plantstart'] = pd.to_datetime(merged_gdf['plantstart'], errors='coerce')\n",
    "merged_gdf['img_date'] = pd.to_datetime(merged_gdf['img_date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: PRE-FILTER IMAGE DATASET (GLOBAL FILTERING)\n",
    "Inputs:\n",
    "- merged: merged dataframe of maxar image metadata + associated polygon metadata\n",
    "\n",
    "Outputs:\n",
    "- filtered_merged: a filtered version of the merged dataframe of maxar image metadata + associated polygon metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4: CREATE DICTIONARY FOR PROJECT-POLYGON MAPPING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 5: ITERATE THROUGH EACH PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 5.1 Get All Polygons for This Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: LOAD AND PREPROCESS DATA\n",
    "# 1.1: Load polygon dataset\n",
    "poly_csv = gpd.GeoDataFrame(polygon geometries & metadata)\n",
    "\n",
    "# 1.2 Load image dataset\n",
    "img_csv = gpd.GeoDataFrame(maxar image geometries & metadata)\n",
    "\n",
    "# 1.3 Preprocess the data\n",
    "poly_gdf = preprocess_polygons(poly_csv) # Clean data, convert geometries, enforce CRS\n",
    "img_gdf = preprocess_images(img_csv) # Clean data, convert geometries, enforce CRS\n",
    "\n",
    "\n",
    "# Step 2: MERGE POLYGON DATA WITH IMAGE DATA\n",
    "merged_gdf = img_gdf.merge(poly_gdf, on=['project_id', 'poly_id'], how='left')\n",
    "\n",
    "# Step 3: PRE-FILTER IMAGES\n",
    "filtered_images = merged_gdf where:\n",
    "    (date is within allowed date range) &\n",
    "    (cloud cover < cloud_thresh) &\n",
    "    (off-nadir angle < off_nadir_thresh) &\n",
    "    (sun elevation < sun_elev_thresh)\n",
    "\n",
    "# Step 4: ITERATE THROUGH PROJECTS AND POLYGONS TO CALCULATE IMAGERY COVERAGE\n",
    "# 4.1 Create a dictionary for project-polygon mapping\n",
    "project_polygons = {project_id: list of poly_ids associated with that project} # Create a dictionary\n",
    "\n",
    "# 4.2 Initialize list to store low coverage cases\n",
    "low_img_coverage_log = []\n",
    "\n",
    "# 4.3 Iterate through each project\n",
    "for each project_id in project_polygons:\n",
    "\n",
    "    # 4.4 Get all polygons for this project\n",
    "    project_polygons_list = list of poly_ids for this project_id\n",
    "\n",
    "    # 4.5 Iterate through each polygon in the project\n",
    "    for each poly_id in project_polygons_list:\n",
    "    \n",
    "        # 4.6 Get all images associated with this polygon\n",
    "        poly_images = filtered_images[filtered_images['poly_id'] == poly_id]\n",
    "\n",
    "        # Count the number of available images\n",
    "        num_images = len(poly_images)\n",
    "\n",
    "        # If no valid image exists, record 0% coverage\n",
    "        if poly_images is empty:\n",
    "            store result: (poly_id, project_id, None, num_images, 0, 0) # No images available\n",
    "            continue\n",
    "\n",
    "        # 4.7 Select the best image (lowest cloud cover)\n",
    "        best_image = select_best_image(poly_images)\n",
    "\n",
    "        # 4.8 Get polygon and image geometries\n",
    "        poly_geom = poly_gdf[poly_gdf['poly_id'] == poly_id].geometry.iloc[0]\n",
    "        best_img_geom = best_image['img_geom']\n",
    "\n",
    "        # 4.9 Compute UTM Zone and reproject geometries\n",
    "        poly_centroid = compute centroid of poly_geom\n",
    "        utm_crs = get UTM CRS from centroid\n",
    "        poly_geom_reprojected = reproject poly_geom to utm_crs\n",
    "        best_img_geom_reprojected = reproject best_img_geom to utm_crs\n",
    "\n",
    "        # 4.10 Calculate the polygon area dynamically (in hectares)\n",
    "        poly_area_ha = poly_geom_reprojected.area / 10000\n",
    "\n",
    "        # 4.11 Calculate area of overlap\n",
    "        overlap_area = poly_geom_reprojected union best_img_geom_reprojected\n",
    "        overlap_area_ha = overlap_area / 10000\n",
    "\n",
    "        # 4.12 Compute percent of polygon area covered\n",
    "        percent_img_cover = (overlap_area / poly_area_ha) * 100\n",
    "\n",
    "        # 4.13 Log cases where imagery coverage is unexpectedly low\n",
    "        if percent_img_cover < 50:\n",
    "            log_entry = {\n",
    "                'poly_id': poly_id,\n",
    "                'project_id': project_id,\n",
    "                'best_image': best_image['title'],\n",
    "                'num_images': num_images,\n",
    "                'poly_area_ha': poly_area_ha,\n",
    "                'overlap_area_ha': overlap_area_ha,\n",
    "                'percent_img_cover': percent_img_cover\n",
    "            }\n",
    "            low_img_coverage_log.append(log_entry)\n",
    "\n",
    "        # 4.14 Store results\n",
    "        store result: (poly_id, project_id, best_image['title'], num_images, poly_area_ha, overlap_area_ha, percent_img_cover)\n",
    "\n",
    "# STEP 5: EXPORT LOW COVERAGE LOG IF NEEDED\n",
    "if low_img_coverage_log is not empty:\n",
    "    export_to_csv(low_img_coverage_log, \"low_coverage_polygons.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##FUNCTIONS LIST \n",
    "\n",
    "# 1. LOAD AND PREPROCESS DATA\n",
    "load_polygons(filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
