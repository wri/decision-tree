{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull Polygons from TerraMatch API\n",
    "\n",
    "This notebook sets up the process to pull polygon geometries and metadata from the TerraMatch API. The steps for pulling polygons are as follows:\n",
    "1. Set up configuration and API token\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import yaml\n",
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up token and API URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up token access\n",
    "auth_path = '../secrets.yaml'\n",
    "with open(auth_path) as auth_file:\n",
    "    auth = yaml.safe_load(auth_file)\n",
    "headers = {\n",
    "    'Authorization': f\"Bearer {auth['access_token']}\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TerraMatch API URLs\n",
    "staging_url = \"https://api-staging.terramatch.org/research/v3/sitePolygons?\" # Use for testing queries\n",
    "prod_url = \"https://api.terramatch.org/research/v3/sitePolygons?\" # Use to pull data for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to pull TM API data\n",
    "def pull_tm_api_data(url, headers, project_ids):\n",
    "    '''\n",
    "    edits to the above function include:\n",
    "        iterating through list of project ids within func so output is a df with \n",
    "        multiple projects\n",
    "        add project id as a column to support maxar metadata request\n",
    "        update to last record variable\n",
    "        added tqdm progress bar  \n",
    "    '''\n",
    "    # List to store all retrieved polygon metadata\n",
    "    results = []\n",
    "    # Set up a progress bar\n",
    "    with tqdm(total=len(project_ids), desc=\"Processing Projects\", unit=\"project\") as progress_bar:\n",
    "        # For every project in the list of project_ids\n",
    "        for project_id in project_ids:\n",
    "            # Set parameters with the current project ID\n",
    "            params = {\n",
    "                'projectId[]': project_id,\n",
    "                'polygonStatus[]': 'approved',\n",
    "                'includeTestProjects': 'false',\n",
    "                'page[size]': '100'\n",
    "            }\n",
    "\n",
    "            last_record = ''\n",
    "            new_last_record = None  # Ensure it's defined before use\n",
    "\n",
    "            while True:\n",
    "                # Send GET request and store the response (polygon geometries & metadata)\n",
    "                response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "                # Check status code\n",
    "                if response.status_code != 200:\n",
    "                    raise ValueError(f'Request failed for project {project_id} with status code {response.status_code}')\n",
    "                \n",
    "                # Convert the response to a JSON and record the total number of records returned\n",
    "                response_json = response.json()\n",
    "                total_records = response_json['meta']['page']['total']\n",
    "\n",
    "                # Parse response data\n",
    "                # If there are no polygons for this project\n",
    "                if total_records == 0:\n",
    "                    break  # Exit if no data is available (skip to the next project)\n",
    "\n",
    "                # Loop through each polygon in the response\n",
    "                for idx in range(0, len(response.json()['data'])):\n",
    "                    # Extract polygon attributes from each record and store them in dictionary data\n",
    "                    data = response_json['data'][idx]['attributes']\n",
    "                    data['poly_id'] = response_json['data'][idx]['meta']['page']['cursor']\n",
    "                    # Store the project_id in data\n",
    "                    data['project_id'] = project_id \n",
    "                    # Append data ( a dictionary of that project's metadata) in the overall results list\n",
    "                    results.append(data)\n",
    "\n",
    "                    # Assign the last cursor only if there are records\n",
    "                    if idx == (total_records - 1):\n",
    "                        new_last_record = response_json['data'][idx]['meta']['page']['cursor']\n",
    "\n",
    "                # Check if there are more pages\n",
    "                if (len(response.json()['data']) == int(params['page[size]'])):\n",
    "                    last_record = new_last_record\n",
    "                    params['page[after]'] = last_record\n",
    "                else:\n",
    "                    break  # Exit pagination if no new cursor is found\n",
    "\n",
    "            progress_bar.update(1) \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create lists of projects to pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in list of approved projects (2025-02-21)\n",
    "full = pd.read_csv('../projects_all_approved_202502211226.csv')\n",
    "full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists of projects by Cohort (and split cohort 1 into projects within the TF landscapes and outside of the TF landscapes)\n",
    "cohort1 = full[full['cohort'] == 'terrafund']\n",
    "cohort1_landscapes = cohort1[cohort1['country'].isin(['BI', 'CD', 'RW', 'KE', 'GH'])]\n",
    "cohort1_non_landscapes = cohort1[~cohort1['country'].isin(['BI', 'CD', 'RW', 'KE', 'GH'])]\n",
    "cohort2 = full[full['cohort'] == 'terrafund-landscapes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of project ids to query\n",
    "ids = list(set(cohort1.project_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a short list of ids for testing\n",
    "ids = ids[:11]\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of ids by specifying project_ids\n",
    "# BirdLife International\n",
    "#birdlife = ['36504a4e-f7a3-4963-9ff2-9aa9982cf990']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull polygons from projects in list of ids from TerraMatch API\n",
    "project_results = pull_tm_api_data(prod_url, headers, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the polygon geometries into a dataframe\n",
    "project_df = pd.DataFrame(project_results)\n",
    "project_df.columns = project_df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(project_df.columns)\n",
    "project_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the polygon geometries & metadata as a csv\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "# project_df.to_csv(f\"../data/tm_api_{today}.csv\", index=False) # To the darby-tm-api-pull repo\n",
    "# project_df.to_csv(f\"/home/darby/github_repos/maxar-tools/data/tm_api_{today}.csv\", index=False) # To the darby-maxar-tools repo\n",
    "\n",
    "\n",
    "# TEST PULL\n",
    "project_df.to_csv(f\"../data/tm_api_DREK_2025-02-26.csv\", index=False) # To the darby-tm-api-pull repo\n",
    "#project_df.to_csv(f\"/home/darby/github_repos/maxar-tools/data/tm_api_TEST_PROD_NEW.csv\", index=False) # To the darby-maxar-tools repo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
