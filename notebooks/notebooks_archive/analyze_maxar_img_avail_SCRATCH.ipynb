{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Analyze Maxar Image Availability\n",
    "\n",
    "Takes in a CSVs of:\n",
    "- All polygon features\n",
    "- All Maxar images available for those polygons\n",
    "- Filtered mage availability per polygon\n",
    "- Filtered image availability per project (aggregated)\n",
    "- Low coverage polygons\n",
    "\n",
    "Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Outline:\n",
    "\n",
    "Step 1: Load already computed CSVs for:\n",
    " - poly_img_avail_base_df\n",
    " - poly_img_avail_ev_df\n",
    " - project_img_avail_base_df\n",
    " - project_img_avail_ev_df\n",
    " - poly_gdf\n",
    " - maxar_gdf\n",
    "\n",
    "Step 2: Merge plantstart years (from poly_gdf) into polygon-level image availability\n",
    "\n",
    "Step 3: Calculate planting year distributions (by polygon)\n",
    "\n",
    "Step 4: Assemble baseline & EV coverage from project-level files\n",
    "\n",
    "Step 5: Calculate % area covered by baseline and by EV imagery\n",
    "\n",
    "Step 6: Calculate % area with coverage at both points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "import image_coverage as img_cover\n",
    "import analyze_img_coverage as analyze\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Set file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input file paths\n",
    "# Polygon metadata & geometries from TM API\n",
    "feats = '../data/tf_cohort1/tm_api_cohort1_2025-04-02.csv' \n",
    "\n",
    "# Metadata for Maxar images corresponding to polygons\n",
    "maxar_md = '../data/tf_cohort1/imagery_availability/comb_img_availability_cohort1_2025-04-02.csv' \n",
    "\n",
    "# List of approved projects (with country codes)\n",
    "approved_projects = '../projects_all_approved_202502211226.csv'\n",
    "\n",
    "# Image availability\n",
    "# Baseline\n",
    "poly_img_avail_base = '../data/tf_cohort1/results/baseline/polygon_imagery_coverage_cohort1_2025-04-02.csv'\n",
    "low_cov_poly_base = '../data/tf_cohort1/results/baseline/low_coverage_polygons_cohort1_2025-04-02.csv'\n",
    "proj_img_avail_base = '../data/tf_cohort1/results/baseline/project_imagery_coverage_cohort1_2025-04-02.csv'\n",
    "\n",
    "# Early Verification\n",
    "poly_img_avail_ev = '../data/tf_cohort1/results/year_2/polygon_imagery_coverage_cohort1_2025-04-02.csv'\n",
    "low_cov_poly_ev = '../data/tf_cohort1/results/year_2/low_coverage_polygons_cohort1_2025-04-02.csv'\n",
    "proj_img_avail_ev = '../data/tf_cohort1/results/year_2/project_imagery_coverage_cohort1_2025-04-02.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Read in files|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polygon feature data\n",
    "poly_df = pd.read_csv(feats)\n",
    "\n",
    "# Maxar data\n",
    "maxar_df = pd.read_csv(maxar_md)\n",
    "\n",
    "# Image availability\n",
    "# Baseline\n",
    "poly_img_avail_base_df = pd.read_csv(poly_img_avail_base)\n",
    "low_cov_poly_base_df = pd.read_csv(low_cov_poly_base)\n",
    "proj_img_avail_base_df = pd.read_csv(proj_img_avail_base)\n",
    "\n",
    "# Early Verification\n",
    "poly_img_avail_ev_df = pd.read_csv(poly_img_avail_ev)\n",
    "low_cov_poly_ev_df = pd.read_csv(low_cov_poly_ev)\n",
    "proj_img_avail_ev_df = pd.read_csv(proj_img_avail_ev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Preprocess polygon and maxar image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_gdf = img_cover.preprocess_polygons(poly_df, debug=True)\n",
    "maxar_gdf = img_cover.preprocess_images(maxar_df, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Merge 'plantstart' and poly_geom into image availability data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge plantstart and poly_geom into baseline and EV image availability\n",
    "poly_img_avail_base_df = poly_img_avail_base_df.merge(poly_gdf[['poly_id', 'plantstart', 'poly_geom']], on='poly_id', how='left')\n",
    "poly_img_avail_ev_df = poly_img_avail_ev_df.merge(poly_gdf[['poly_id', 'plantstart', 'poly_geom']], on='poly_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_img_avail_base_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Calculate planting year stats by project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year from plantstart\n",
    "poly_img_avail_base_df['plant_year'] = poly_img_avail_base_df['plantstart'].dt.year\n",
    "\n",
    "# Group by project and calculate planting year distinctions\n",
    "planting_stats = poly_img_avail_base_df.groupby('project_id').agg(\n",
    "    num_poly=('poly_id', 'count'),\n",
    "    pct_poly_plant_2022=('plant_year', lambda x: (x == 2022).sum() / len(x) * 100),\n",
    "    pct_poly_plant_2023=('plant_year', lambda x: (x == 2023).sum() / len(x) * 100),\n",
    "    pct_poly_plant_2024=('plant_year', lambda x: (x == 2024).sum() / len(x) * 100),\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Calculate % polygons with imagery at baseline & early verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count % of polygons with at least one baseline image\n",
    "poly_img_avail_base_df['has_base_img'] = poly_img_avail_base_df['num_images'] > 0\n",
    "base_img_stats = poly_img_avail_base_df.groupby('project_id')['has_base_img'].mean().reset_index()\n",
    "base_img_stats = base_img_stats.rename(columns={'has_base_img': 'pct_poly_wi_base_img'})\n",
    "base_img_stats['pct_poly_wi_base_img'] *= 100\n",
    "\n",
    "# Count # of polygons with at least one early verification image\n",
    "poly_img_avail_ev_df['has_ev_img'] = poly_img_avail_ev_df['num_images'] > 0\n",
    "ev_img_stats = poly_img_avail_ev_df.groupby('project_id')['has_ev_img'].mean().reset_index()\n",
    "ev_img_stats = ev_img_stats.rename(columns={'has_ev_img': 'pct_poly_wi_ev_img'})\n",
    "ev_img_stats['pct_poly_wi_ev_img'] *= 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Calculate % of polygons with high image coverage (> 70%) at both time points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only needed columns\n",
    "base_cov = poly_img_avail_base_df[['poly_id', 'project_id', 'percent_img_cover']].copy()\n",
    "ev_cov = poly_img_avail_ev_df[['poly_id', 'percent_img_cover']].copy()\n",
    "\n",
    "# Rename for clarity before merge\n",
    "base_cov = base_cov.rename(columns={'percent_img_cover': 'base_pct_img_cover'})\n",
    "ev_cov = ev_cov.rename(columns={'percent_img_cover': 'ev_pct_img_cover'})\n",
    "\n",
    "# Merge coverage values by poly_id\n",
    "joined_cov = base_cov.merge(ev_cov, on='poly_id', how='inner')\n",
    "\n",
    "# Define a coverage threshold for \"high coverage\"\n",
    "cover_thresh = 70\n",
    "\n",
    "# Check if both timepoints have > 70% coverage\n",
    "joined_cov['high_cov_both'] = (\n",
    "    (joined_cov['base_pct_img_cover'] >= cover_thresh) &\n",
    "    (joined_cov['ev_pct_img_cover'] >= cover_thresh)\n",
    ")\n",
    "\n",
    "# Group by project and compute % of polygons with high coverage at both\n",
    "high_cov_stats = joined_cov.groupby('project_id')['high_cov_both'].mean().reset_index()\n",
    "high_cov_stats['pct_poly_wi_high_cov_both'] = high_cov_stats['high_cov_both'] * 100\n",
    "high_cov_stats = high_cov_stats.drop(columns='high_cov_both')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Assemble summary table at project level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from planting_stats (project_id, number of polygons, and % that started planting each year)\n",
    "project_summary_df = planting_stats.copy()\n",
    "\n",
    "# Merge in baseline image availability\n",
    "project_summary_df = project_summary_df.merge(base_img_stats, on='project_id', how='left')\n",
    "\n",
    "# Merge in early verification image availability\n",
    "project_summary_df = project_summary_df.merge(ev_img_stats, on='project_id', how='left')\n",
    "\n",
    "# Merge in % polygons with high coverage at both time points\n",
    "project_summary_df = project_summary_df.merge(high_cov_stats, on='project_id', how='left')\n",
    "\n",
    "project_summary_df = project_summary_df.sort_values(by='pct_poly_wi_high_cov_both', ascending=False)\n",
    "project_summary_df.sort_values('pct_poly_plant_2022', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do earlier planting projects tend to have higher imagery coverage?\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cos_to_check = [\n",
    "    'pct_poly_plant_2022',\n",
    "    'pct_poly_plant_2023',\n",
    "    'pct_poly_plant_2024',\n",
    "    'pct_poly_wi_base_img',\n",
    "    'pct_poly_wi_ev_img',\n",
    "    'pct_poly_wi_high_cov_both'\n",
    "]\n",
    "\n",
    "sns.heatmap(project_summary_df[cos_to_check].corr(), annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation between Planting Years and Imagery Coverage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=project_summary_df, x='pct_poly_plant_2022', y='pct_poly_wi_ev_img')\n",
    "plt.title(\"2022 Planting vs EV Imagery Coverage\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(data=project_summary_df, x='pct_poly_plant_2022', y='pct_poly_wi_high_cov_both')\n",
    "plt.title(\"2022 Planting vs Shared Imagery Coverage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "x = project_summary_df['pct_poly_plant_2022']\n",
    "y = project_summary_df['pct_poly_wi_ev_img']\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "model = sm.OLS(y, x).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group projects by dominant planting year\n",
    "def dominant_planting_year(row):\n",
    "    return max(('2022', row['pct_poly_plant_2022']), \n",
    "               ('2023', row['pct_poly_plant_2023']),\n",
    "               ('2024', row['pct_poly_plant_2024']), \n",
    "               key=lambda x: x[1])[0]\n",
    "\n",
    "project_summary_df['dominant_year'] = project_summary_df.apply(dominant_planting_year, axis=1)\n",
    "project_summary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=project_summary_df, x='dominant_year', y='pct_poly_wi_high_cov_both')\n",
    "plt.title('Imagery Coverage by Dominant Planting Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_summary_df.groupby('dominant_year')['pct_poly_wi_high_cov_both'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_summary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag High/Low Coverage Projects\n",
    "project_summary_df['high_coverage'] = project_summary_df['pct_poly_wi_high_cov_both'] >= 75\n",
    "sns.boxplot(data=project_summary_df, x='high_coverage', y='pct_poly_plant_2022')\n",
    "plt.title(\"2022 Planting Share in High vs. Low Coverage Projects\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_summary_df.groupby('dominant_year')['pct_poly_wi_ev_img'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_summary_df.groupby('dominant_year')[['pct_poly_wi_ev_img', 'pct_poly_wi_high_cov_both']].agg(['mean', 'median', 'std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Calculate % of project area with imagery at baseline and early verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_img_id(title):\n",
    "    \"\"\"\n",
    "    Extracts the Maxar image ID from the Maxar image title \n",
    "    \"\"\"\n",
    "    if isinstance(title, str) and title.startswith(\"Maxar\"):\n",
    "        return title.split()[-1]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### Create a dataframe with best image id and geometry for all polygons for baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an 'img_id' column to the dataframes of polygon-level best image availability at baseline & EV\n",
    "poly_img_avail_base_df['img_id_base'] = poly_img_avail_base_df['best_image'].apply(extract_img_id)\n",
    "poly_img_avail_ev_df['img_id_ev'] = poly_img_avail_ev_df['best_image'].apply(extract_img_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with each unique img_id (and it associated geometry) from maxar_gdf\n",
    "img_geom_lookup = maxar_gdf[['img_id', 'img_geom']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the image's footprint geometry (img_geom) to each relevent row in poly_img_avail_base/ev_df\n",
    "\n",
    "# Baseline\n",
    "poly_img_avail_base_df = poly_img_avail_base_df.merge(\n",
    "    img_geom_lookup.rename(columns={'img_id': 'img_id_base', 'img_geom': 'img_geom_base'}),\n",
    "    on='img_id_base', how='left'\n",
    ")\n",
    "\n",
    "# Early verification\n",
    "poly_img_avail_ev_df = poly_img_avail_ev_df.merge(\n",
    "    img_geom_lookup.rename(columns={'img_id': 'img_id_ev', 'img_geom': 'img_geom_ev'}),\n",
    "    on='img_id_ev', how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build a dataframe with all relevent info for the best image at baseline & EV for each polygon ##\n",
    "\n",
    "# Step 1: Select relevent columns from baseline dataframe and rename\n",
    "base_cols = poly_img_avail_base_df[[\n",
    "    'poly_id', 'project_id', 'plantstart', 'poly_geom',\n",
    "    'best_image', 'img_id_base', 'percent_img_cover', 'img_geom_base'\n",
    "]].rename(columns={\n",
    "    'best_image': 'best_image_base',\n",
    "    'percent_img_cover': 'percent_img_cover_base',\n",
    "})\n",
    "\n",
    "# Step 2: Select relevent columns from early verification dataframe and rename\n",
    "ev_cols = poly_img_avail_ev_df[[\n",
    "    'poly_id', 'best_image', 'img_id_ev', 'percent_img_cover', 'img_geom_ev'\n",
    "]].rename(columns={\n",
    "    'best_image': 'best_image_ev',\n",
    "    'percent_img_cover': 'percent_img_cover_ev',\n",
    "})\n",
    "\n",
    "# Step 3: Merge baseline and EV info on poly_id (inner)\n",
    "#  This creates a DF with ONLY polygons with a best image at both baseline & EV\n",
    "poly_double_cov_df = base_cols.merge(ev_cols, on='poly_id', how='inner')\n",
    "\n",
    "print(f\"Length of poly_double_cov_df before filtering: {len(poly_double_cov_df)}\")\n",
    "\n",
    "# Step 4: Filter to only include polygons with a best image at both baseline & EV\n",
    "poly_double_cov_df = poly_double_cov_df.dropna(subset=['best_image_base', 'best_image_ev'])\n",
    "print(f\"Length of poly_double_cov_df after filtering: {len(poly_double_cov_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #53 - very small overlap\n",
    "# #117\n",
    "# test_row = poly_double_cov_df.iloc[117]\n",
    "# print(\"Test row:\")\n",
    "# print(test_row)\n",
    "# print()\n",
    "\n",
    "# # Extract geometries\n",
    "# poly_geom = test_row['poly_geom']\n",
    "# img_geom_base = test_row['img_geom_base']\n",
    "# img_geom_ev = test_row['img_geom_ev']\n",
    "\n",
    "# # Get UTM CRS from polygon centroid\n",
    "# centroid = poly_geom.centroid\n",
    "# utm_crs = img_cover.get_utm_crs(centroid.x, centroid.y)\n",
    "# print(f\"Using UTM CRS: {utm_crs}\")\n",
    "\n",
    "# # Reproject all geometries to UTM\n",
    "# poly_proj = gpd.GeoSeries([poly_geom], crs=\"EPSG:4326\").to_crs(utm_crs).iloc[0]\n",
    "# base_proj = gpd.GeoSeries([img_geom_base], crs=\"EPSG:4326\").to_crs(utm_crs).iloc[0]\n",
    "# ev_proj = gpd.GeoSeries([img_geom_ev], crs=\"EPSG:4326\").to_crs(utm_crs).iloc[0]\n",
    "\n",
    "# # Calculate polygon area (ha)\n",
    "# poly_area_ha = poly_proj.area / 10_000\n",
    "# print(f\"Polygon area: {poly_area_ha:.2f} ha\")\n",
    "\n",
    "# # Get overlaps\n",
    "# overlap_base = poly_proj.intersection(base_proj)\n",
    "# overlap_ev = poly_proj.intersection(ev_proj)\n",
    "\n",
    "# # Get intersection of overlaps\n",
    "# overlap_both = overlap_base.intersection(overlap_ev)\n",
    "\n",
    "# # Compute shared coverage area (ha)\n",
    "# overlap_area_ha = overlap_both.area / 10_000\n",
    "# percent_overlap = (overlap_area_ha / poly_area_ha) * 100\n",
    "\n",
    "# # Print results\n",
    "# print()\n",
    "# print(f\"Overlap with both images: {overlap_area_ha:.6f} ha\")\n",
    "# print(f\"Percent polygon area with shared image coverage: {percent_overlap:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_shared_image_overlap(row, debug=False):\n",
    "    \"\"\"\n",
    "    Given a row with polygon and baseline/EV image footprints, compute shared area of image coverage\n",
    "    (in hectares and as a % of the polygon's area)\n",
    "    \"\"\"\n",
    "    # Extract geometries\n",
    "    poly_geom = row['poly_geom']\n",
    "    base_img = row['img_geom_base']\n",
    "    ev_geom = row['img_geom_ev']\n",
    "\n",
    "    # Use centroid to determine UTM zone\n",
    "    centroid = poly_geom.centroid\n",
    "    utm_crs = img_cover.get_utm_crs(centroid.x, centroid.y)\n",
    "\n",
    "    # Reproject all geometries to UTM\n",
    "    poly_proj = gpd.GeoSeries([poly_geom], crs=\"EPSG:4326\").to_crs(utm_crs).iloc[0]\n",
    "    base_proj = gpd.GeoSeries([img_geom_base], crs=\"EPSG:4326\").to_crs(utm_crs).iloc[0]\n",
    "    ev_proj = gpd.GeoSeries([img_geom_ev], crs=\"EPSG:4326\").to_crs(utm_crs).iloc[0]\n",
    "\n",
    "    # Compute area of polygon\n",
    "    poly_area_ha = poly_proj.area / 10_000\n",
    "\n",
    "    # Calculate overlap between polygon and imagery at baseline & EV\n",
    "    overlap_base = poly_proj.intersection(base_proj)\n",
    "    overlap_ev = poly_proj.intersection(ev_proj)\n",
    "\n",
    "    # Get intersection of overlaps\n",
    "    shared_overlap = overlap_base.intersection(overlap_ev)\n",
    "\n",
    "    # Calculate area of shared overlap & % of polygon area\n",
    "    shared_overlap_area_ha = shared_overlap.area / 10_000\n",
    "    shared_pct_cover = (shared_overlap_area_ha / poly_area_ha) * 100 if poly_area_ha > 0 else 0\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Polygon area (ha): {poly_area_ha:.2f}\")\n",
    "        print(f\"Shared overlap area (ha): {shared_overlap_area_ha:.4f}\")\n",
    "        print(f\"Shared coverage (%): {shared_pct_cover:.2f}\")\n",
    "\n",
    "    return pd.Series({\n",
    "        'poly_area_ha_actual': poly_area_ha,\n",
    "        'shared_overlap_ha': shared_overlap_area_ha,\n",
    "        'shared_pct_img_cover': shared_pct_cover\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the compute_shared_image_overlap() function to all rows of the poly_double_cov_df dataframe\n",
    "shared_cov_results = poly_double_cov_df.apply(\n",
    "    lambda row: compute_shared_image_overlap(row, debug=False), axis=1\n",
    ")\n",
    "\n",
    "# Merge the results into the original dataframe (of all polygons with images at both baseline & EV)\n",
    "poly_double_cov_df = pd.concat([poly_double_cov_df, shared_cov_results], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_double_cov_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_double_cov_df['shared_pct_img_cover'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(poly_double_cov_df[poly_double_cov_df['shared_pct_img_cover'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(poly_double_cov_df[(poly_double_cov_df['percent_img_cover_base'] == 100) &\n",
    "                   (poly_double_cov_df['percent_img_cover_ev'] == 100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_double_cov_df[['percent_img_cover_base', 'percent_img_cover_ev', 'shared_pct_img_cover']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_row = poly_double_cov_df.iloc[0]\n",
    "test_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compute_shared_image_overlap(test_row)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
